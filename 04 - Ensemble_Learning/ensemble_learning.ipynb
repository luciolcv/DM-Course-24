{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning \n",
    "\n",
    "The intuition underlying the following methods is simple, yet strong.\n",
    "\n",
    "It is an adaptation to the notion of *wisdom of crowd*.\n",
    "That is, if you ask a complex question to many randomly selected people,\n",
    "and then aggregate their answers, chances are that this cumulative answer\n",
    "will be as good as the one an expert would provide to you.\n",
    "\n",
    "In machine learning this approach is called *Ensemble Learning*, while a learning algorithm is called\n",
    "*ensemble method*.\n",
    "\n",
    "**Example**\n",
    "You have a group of Decision Tree classifiers, each one trained upon a different\n",
    "subset of the entire training set.\n",
    "\n",
    "Once every tree has completed its training, you can carry out a prediciton as the aggregation\n",
    "of the outcomes provided by every tree.\n",
    "\n",
    "For instance, you can set up a voting process. The class that has been given with the majority of \n",
    "the votes represents the final decision.\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifiers\n",
    "You have trained a collection of classifiers. \n",
    "Any classifier in this collection has accuracy around 80%.\n",
    "\n",
    "For instance, in your collection there is a Log. Regr. Classifier,\n",
    "an SVM classifier, a Random Forest Classifier and a K-Neigh. Classifier\n",
    "\n",
    "\n",
    "![img/voting.png](img/voting.png)\n",
    "\n",
    "\n",
    "One way to aggregate aggregate these classifiers is through a voting system.\n",
    "\n",
    "There are two possible approaches:\n",
    "\n",
    "\n",
    "\n",
    "1. *Hard Voting* - You compute the argmax \n",
    "2. *Soft Voting* - You compute a weighted argmax\n",
    "\n",
    "**Why does this approach work?**\n",
    "Suppose you have a biased coin. With 51% of probability of coming up head.\n",
    "\n",
    "If you toss it 1000 time you will have that approximately 75% of the times\n",
    "the coin came up head.\n",
    "\n",
    "If you toss it 10000 time you will see that the probability of seeing \n",
    "head get closer and closer to its actual value, i.e., 51%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "heads_proba = 0.51\n",
    "coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\n",
    "cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3.5))\n",
    "plt.plot(cumulative_heads_ratio)\n",
    "plt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\n",
    "plt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\n",
    "plt.xlabel(\"Number of coin tosses\")\n",
    "plt.ylabel(\"Heads ratio\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 10000, 0.42, 0.58])\n",
    "# save_fig(\"law_of_large_numbers_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the approach works because trials are independent from one another.\n",
    "\n",
    "In fact, this independence is a fundamental requirement in order this approach to work best.\n",
    "\n",
    "**Question**\n",
    "Is this requirement always satisfied?\n",
    "\n",
    "What does *independence* mean in our context?\n",
    "\n",
    " - We need to reduce the correlation between classifiers\n",
    "\n",
    "---\n",
    "\n",
    "**Voting classifier in sklearn**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to import the classifiers you want to include in you ensamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# instantiate the objects\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "# insert each estimator in the ensamble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confront the accuracy of each weak learner upon the test set, and compare these\n",
    "results with the ones achieved by the ensemble learner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the classifiers in the ensamble estimate class probabilities (the have the ``predict_proba()`` method) you can\n",
    "adopt a soft-voting strategy. \n",
    "\n",
    "All you need to do is to replace ``voting=hard`` to ``voting=soft``.\n",
    "\n",
    "**Try it yourself**\n",
    "Note that the ``SVC`` classifier does not compute probabilities by default. But, if you set\n",
    "the argument ``probability=True`` it will perform a cross validation test during training to estimate the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "Obviously, with a voting approach, the only way to reduce the correlation between every weak learner errors,\n",
    "is to select very different learners.\n",
    "\n",
    "\n",
    "However, there is another approach, i.e., by using either bagging or pasting.\n",
    "\n",
    "The common idea is to use a single algorithm -- instantiated multiple times -- and feed it with different \n",
    "subsets of data, drawn from the entire training set.\n",
    "\n",
    "This strategy can be implemented in two ways:\n",
    "\n",
    "1. Bagging - short for bootstrap aggregating, sampling __with replacement__\n",
    "2. Pasting - sampling __without replacement__.\n",
    "\n",
    "In other words, baggin may produce overlapping subsets, while pasting, conversely, produces non-overlapping samples of the training set.\n",
    "\n",
    "\n",
    "![img/bagging.png](img/bagging.png)\n",
    "\n",
    "Again, once every weak learner finishes its training, prediction \n",
    "are made by aggregating the outcome of every weak learner.\n",
    "\n",
    "Most common approach is:\n",
    "\n",
    "1. most frequent prediciton (like hard voting) for classification tasks\n",
    "2. average for regression tasks\n",
    "\n",
    "It should be noted that, weak learners have higher bias -- if compared with the case where they can have\n",
    "access to the entire dataset -- however the aggregation is able to reduce this bias.\n",
    "\n",
    "Generally speaking, the ensemble resulting by either bagging or pasting, has a bias that is similar\n",
    "to the one of its weak learner, by the variance of the ensemble is significantly lower.\n",
    "\n",
    "\n",
    "### Bagging and Pasting in Scikit-Learn\n",
    "\n",
    "Scikit-Learn offers a simple API for both bagging and pasting with the ``BaggingClassifier`` class (or ``BaggingRegressor`` for regression). \n",
    "\n",
    "The following code trains an ensemble of 500 Decision Tree classifiers.\n",
    "\n",
    "Each  tree is trained  upon 100 samples randomly selected from the training set, with \n",
    "replacement (otherwise, you can set ``bootstrap=False`` to \n",
    "use a pasting approach).\n",
    "\n",
    "The ``n_jobs`` parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use all available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Ensamble accuracy: \",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(\"Single learner accuracy\", accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision boundaries of each classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "#save_fig(\"decision_tree_without_and_with_bagging_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \n",
    "the ensemble has smoother boundaries, as a consequence \n",
    "it can be far superior in terms of generalization.\n",
    "\n",
    "\n",
    "The ensemble has a bias that is similar to the one of the weak learner, while\n",
    "the variance is lower. This means that, with bagging, performance on the \n",
    "training set are roughly equivalent to the one of the weak learner, but it\n",
    "behaves significantly better on the test set.\n",
    "\n",
    "Therefore, bagging (sampling with replacement) has a \n",
    "higher bias and a lower variance compared to pasting.\n",
    "In fact, in bagging the single predictors are less correlated\n",
    "with each other.\n",
    "\n",
    "Overall, bagging often results in better models, which explains why it is generally preferred over pasting.\n",
    "\n",
    "### Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. \n",
    "\n",
    "By default a ``BaggingClassifier`` samples ``m`` training instances with replacement (``bootstrap=True``). \n",
    "\n",
    "This means that there is a considerable amount of instances that are not sampled at all, thus they are\n",
    "used to train any model. \n",
    "\n",
    "These instances are called out-of-bag (oob) instances.\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated against these instances, \n",
    "without the need for a separate validation set.\n",
    "\n",
    "You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "\n",
    "In other words, the out-of-bag evaluation is __somehow__ similar to the cross validation. \n",
    "\n",
    "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score is available through the oob_score_ variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "                DecisionTreeClassifier(), \n",
    "                n_estimators=500,\n",
    "                bootstrap=True, \n",
    "                n_jobs=-1,\n",
    "                oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oob score roughly resembles the error on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces\n",
    "\n",
    "The ``BaggingClassifier`` class supports sampling the features as well. \n",
    "\n",
    "\n",
    "This behavior is controlled by two hyperparameters: ``max_features`` and ``bootstrap_features``. \n",
    "\n",
    "They are similar to  ``max_samples`` and ``bootstrap``, the only difference is that they apply to \n",
    "the feature selection process.\n",
    "\n",
    "Thus, each predictor will be trained on a random subset of the input features.\n",
    "\n",
    "\n",
    "The approach consisting of sampling both training instances and features is referred as **Random Patches**.\n",
    "\n",
    "While the approach consisting of sampling only from the features space, while accounting for the entire\n",
    "training set is called **Random Subspaces**.\n",
    "\n",
    "Sampling features results in even more predictor diversity, therefore lower variance higher bias.\n",
    "\n",
    "__Exercise__\n",
    "Try a bagging classifier with random patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "A Random Forest is an ensemble of Decision Trees.\n",
    "\n",
    "Usually, training is performed with bagging (it bootstraps, sampling with replacement).\n",
    "\n",
    "With ``max_samples`` you set the maximum number of points within each training set.\n",
    "\n",
    "Similarly, there is a RandomForestRegressor class for regression tasks.\n",
    "\n",
    "The following code trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, a  ``RandomForestClassifier`` has all the hyper-parameters of a ``DecisionTreeClassifier`` (to control how trees are grown),\n",
    "plus all the hyperparameters of a \n",
    "``BaggingClassifier`` to control the ensemble itself.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees:\n",
    "instead of searching for the very best feature when splitting a node\n",
    "it searches for the best feature among a random subset of features. \n",
    "\n",
    "This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally producing a better model.\n",
    "The following ``BaggingClassifier`` is roughly equivalent to the previous ``RandomForestClassifier``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "Implement a random forest classifier using a ``BaggingClassifier``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# implement it via the BaggingClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.sum(y_pred == y_pred_rf) / len(y_pred)  # almost identical predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "\n",
    "Random Forest are also useful when one wants to estimate the importance of every feature \n",
    "in the dataset.\n",
    "\n",
    "Importance is established by looking at the average impact of each feature in terms of impurity reduction.\n",
    "\n",
    "More specifically, each spit of the data is associated with a feature and represented with a node.\n",
    "The feature importance of a feature $f$ is the weighted by the average, where the weight is denoted \n",
    "by the number of training instances contained in a certain node, of the impurity reduction of every split \n",
    "$f$ is involved in.\n",
    "\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. \n",
    "\n",
    "You can access the result using the ``feature_importances_`` variable. \n",
    "\n",
    "For example, the following code trains a ``RandomForestClassifier`` on the iris dataset and outputs each feature’s importance. \n",
    "\n",
    "It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains 15 ``DecisionTreeClassifier``, each one of them is given with a different portion of the training set.\n",
    "\n",
    "After every learning episode the decision boundaries are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for i in range(15):\n",
    "    tree_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i)\n",
    "    indices_with_replacement = np.random.randint(0, len(X_train), len(X_train))\n",
    "    tree_clf.fit(X[indices_with_replacement], y[indices_with_replacement])\n",
    "    plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.02, contour=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "Random forest are very useful, not only for their prediction, but also for their ability \n",
    "to provide an understanding about the important features within a dataset.\n",
    "\n",
    "In fact, one should always consider the adoption of a random forest classifier in order to extract the most important features\n",
    "of a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Boosting\n",
    "Boosting is another ensemble method, that combines several weak learners.\n",
    "\n",
    "It is based on an iterative and incremental learning strategy. \n",
    "At each iteration, a weak learner is trained trying to correct the errors\n",
    "made by the previous learner.\n",
    "\n",
    "There are many boosting methods available, but by far the most popular are AdaBoost (short for Adaptive Boosting) and Gradient Boosting. Let’s start with AdaBoost.\n",
    "\n",
    "### AdaBoost\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted.\n",
    "\n",
    "This this approach, each leaner focuses on the errors on its  predecessor.\n",
    "\n",
    "This is the method adopted in ``AdaBoost``\n",
    "\n",
    "With ``AdaBoost``, first a base classifier is trained. \n",
    "Then, accordingly to the predictions of this classifier, each data point is associated with a weight, whose\n",
    "value is proportional to the error made by the algorithm.\n",
    "\n",
    "In the next iteration, a new classifier is trained upon the weighted dataset and then the weights are updated\n",
    "for the following iteration.\n",
    "\n",
    "Here is a schema of this approach.\n",
    "\n",
    "![img/adaboost.png](img/adaboost.png)\n",
    "\n",
    "\n",
    "Once all predictors are trained, prediction are carried with a soft-voting mechanism,\n",
    "where each learner has a weight proportional to its accuracy.\n",
    "\n",
    "\n",
    "**Question**\n",
    "There is a major drawback with this technique. Any idea?\n",
    "\n",
    "*Hint: parallelism/scalability*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(ada_clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularized SVM classifier with an RBF kernel). \n",
    "\n",
    "- The first classifier gets many instances wrong, so their weights get boosted. \n",
    "\n",
    "- The second classifier therefore does a better job on these instances, and so on.\n",
    "\n",
    "The plot on the right represents the same sequence of predictors except that the learning rate is halved (i.e., the misclassified instance weights are boosted half as much at every iteration). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for subplot, learning_rate in ((121, 1), (122, 0.5)):\n",
    "    sample_weights = np.ones(m)\n",
    "    plt.subplot(subplot)\n",
    "    for i in range(5):\n",
    "        svm_clf = SVC(kernel=\"rbf\", C=0.05, gamma=\"scale\", random_state=42)\n",
    "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        y_pred = svm_clf.predict(X_train)\n",
    "        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n",
    "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
    "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
    "    if subplot == 121:\n",
    "        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n",
    "        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n",
    "        plt.text(-0.5,  0.10, \"3\", fontsize=14)\n",
    "        plt.text(-0.4,  0.55, \"4\", fontsize=14)\n",
    "        plt.text(-0.3,  0.90, \"5\", fontsize=14)\n",
    "\n",
    "# save_fig(\"boosting_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of the internal variables of AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "list(m for m in dir(ada_clf) if not m.startswith(\"_\") and m.endswith(\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a closer look**\n",
    "At the beginning, each data point in the training set $\\mathcal{D}$ has the same weight,\n",
    "for instance $w^{(i)} = \\frac{1}{m}$ for \n",
    "every $x^{(i)} \\in \\mathcal{D}$ (m is the size of the training set).\n",
    "\n",
    "After the first training episode, a weighted error is computed for each data point in the training set. \n",
    "\n",
    "The weighted error rate of the j-th predictor is is given by:\n",
    "\n",
    "$$\n",
    "r_j = \\frac{\\sum_{\\forall i  \\text{ s.t. } \\hat{y^{(i)}} \\neq y^{(i)} }^{m} w^{(i)}}{\\sum_{i=1}^m w^{(i)}}\n",
    "$$\n",
    "\n",
    "This score is then used to establish the weight of the classifiers, according to the\n",
    "following function:\n",
    "\n",
    "$$\n",
    "\\alpha_j = \\eta \\log \\frac{1-r_j}{r_j}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate (1 by default).\n",
    "\n",
    "Clearly, the weight is proportional to the accuracy of a classifier.\n",
    "\n",
    "\n",
    "Then, at end of the next training episode, the weights are updated \n",
    "according to the folling equation:\n",
    "$$\n",
    "w^{(i)} = \\left\\{ \\begin{array}{ll}\n",
    "                    w^{(i)} & \\text{if } \\hat{y^{(i)}} = y^{(i)} \\\\\n",
    "                    w^{(i)}  e^{\\alpha_j} & \\text{otherwise}\n",
    "                  \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "At the beginning of each training episode, weights are normalized, i.e., they are divided by $\\sum_{i=1}^{m} w^{(i)}$.\n",
    "\n",
    "This process is repeated iteratively until either the predefined number of predictors is reached or the perfect predictor is found.\n",
    "\n",
    "Finally any prediction is made according to the following rule:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\text{arg}\\!\\max_{k} \\sum_{j=1\\\\\\hat{y_j}(x) = k}^{N} \\alpha_j\n",
    "$$\n",
    "\n",
    "It is the class that received most votes (weighted by the $\\alpha$ score of each predictor).\n",
    "\n",
    "Where $N$ is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Another very popular Boosting algorithm is Gradient Boosting. \n",
    "\n",
    "Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one is built on top of its predecessor. \n",
    "\n",
    "However, instead of assigning weights to data points, at every iteration \n",
    "GB tries to fit the errors/residual of the previous training episode.\n",
    "\n",
    "Let’s go through a simple regression example using Decision Trees as the base predictors.\n",
    "\n",
    "This is called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT).\n",
    "First, let’s fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "np.random.seed(42)\n",
    "\n",
    "# train the first predictor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "# compute the residual and train the second predictor\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "# compute the residual and train the third predictor\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees.\n",
    "It can make predictions on a new instance simply by adding up the predictions of all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "X_new = np.array([[0.8]])\n",
    "y_new = 3*X_new**2 + 0.05 * np.random.randn(1)\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figures represent the predictions of these trees in the left column,\n",
    "vs the ensemble’s predictions in the right column. \n",
    "\n",
    "In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. \n",
    "\n",
    "In the second row, a new tree is trained on the residual errors of the first tree. \n",
    "On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. \n",
    "\n",
    "\n",
    "Similarly, in the third row another tree is trained on the residual errors of the second tree.\n",
    "\n",
    "\n",
    "You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)\n",
    "\n",
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Ensemble predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "# save_fig(\"gradient_boosting_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course sklearn provides a class for this kind of ensemble.\n",
    "\n",
    "One important parameter is the  ``learning_rate``.\n",
    "If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set,\n",
    "but the predictions will usually generalize better. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure shows two GBRT ensembles trained with a low learning rate: the one on the left does not have enough trees to fit the training set, while the one on the right has too many trees and overfits the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
    "\n",
    "# save_fig(\"gbrt_learning_rate_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting with Early stopping\n",
    "\n",
    "In order to find the optimal number of trees, you can use a technique called _early stopping_.\n",
    "\n",
    "A simple way to implement it, is via the  ``staged_predict()`` method, which returns\n",
    "an iterator upon all the predictions made by the at each stage of training.\n",
    "\n",
    "\n",
    "The following code trains a GBRT ensemble with 120 trees, \n",
    "then measures the validation error at each stage of training \n",
    "to find the optimal number of trees.\n",
    "Finally it trains another \n",
    "GBRT ensemble using the optimal number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# use staged_predict for getting a sequence of predictions\n",
    "# !!!!!!!!! THE EVALUATION MUST BE DONE OUT-SAMPLE (ON THE VALIDATION SET) !!!!!!!!!!!\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "# get the number of tree corresponding to the minimum error\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "# re-train the regressor\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "min_error = np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(132)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "\n",
    "# save_fig(\"early_stopping_gbrt_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement an early stopping technique is\n",
    "to specify   ``warm_start=True``.\n",
    "\n",
    "The following code stop training when the validation error does not improve for five iterationin a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting provide also an additional option, called ``subsample``. \n",
    "\n",
    "For instance, when ``subsample=.25``, it means that each tree in the ensamble is trained against\n",
    "the  25% of the entire training set. \n",
    "\n",
    "Of course, in this way you are introducing more variance.\n",
    "\n",
    "This technique is often referred as **Stochastic Gradient Boosting**.\n",
    "\n",
    "It is generally faster than the previous solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XGBoost\n",
    "It is an optimized implementation of gradient boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost\n",
    "except ImportError as ex:\n",
    "    print(\"Error: the xgboost library is not installed.\")\n",
    "    xgboost = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if xgboost is not None:  \n",
    "    xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    y_pred = xgb_reg.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred) \n",
    "    print(\"Validation MSE:\", val_error)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if xgboost is not None: \n",
    "    xgb_reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "    y_pred = xgb_reg.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred) \n",
    "    print(\"Validation MSE:\", val_error)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%timeit xgboost.XGBRegressor().fit(X_train, y_train) if xgboost is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not guarantee a boost in performance in terms of effectiveness, but it does provide a boost in performance in terms of efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensemble\n",
    "Stacking stands for Stacked generalization.\n",
    "\n",
    "It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, \n",
    "why don’t we train a model to perform this aggregation? \n",
    "\n",
    "Figure shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction (3.0).\n",
    "\n",
    "![img/stacking1.png](img/stacking1.png)\n",
    "\n",
    "\n",
    "To train the *blender*, a common approach is to use a hold-out set. \n",
    "\n",
    "---\n",
    "\n",
    "Let’s see how it works. First, the training set is split in two subsets. \n",
    "\n",
    "> **The first subset**. \n",
    "    The first subset is used to train the predictors in the first layer.\n",
    "    Next, the first layer predictors are used to make predictions on the second (held-out) set. This\n",
    "    ensures that the predictions are “clean,” since the predictors never saw these instances during training.\n",
    "\n",
    "![img/stacking2.png](img/stacking2.png)\n",
    "\n",
    "> **The second subset**. It is created starting from the predictions made by the estimators trained wrt the first subset of training samples.\n",
    " Now for each instance in the hold-out set there are three predicted values. We can create     a new training set using these predicted values as input features (which makes this new training set \n",
    " three-dimensional), and keeping the target values. The blender is trained on this new training \n",
    " set, so it learns to predict the target value given the first layer’s predictions.\n",
    "\n",
    "\n",
    "![img/stacking3.png](img/stacking3.png)\n",
    "\n",
    "\n",
    "It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression, and so on): we get a whole layer of blenders. The trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer (using predictions made by the predictors of the second layer). Once this is done, we can make a prediction for a new instance by going through each layer sequentially, as shown in Figure.\n",
    "\n",
    "\n",
    "Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own implementation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "name": "ensamble_learning.ipynb",
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
