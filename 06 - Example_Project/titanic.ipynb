{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Walkthrough\n",
    "This lecture provides some useful guidelines to follow throughout\n",
    "the development of the homework project.\n",
    "\n",
    "## Titanic, Learning from the disaster\n",
    "\n",
    "---\n",
    "__Suggestions__:\n",
    "\n",
    "1. Define a clear problem statement, which reflects the ultimate goal of your analysis.\n",
    "2. What is the nature of your problem (supervised/unsupervised, clustering, online/offline learning)?\n",
    "3. Do research for finding any solution to the problem in hand. Use previous experience as a tool.\n",
    "4. Define a performance metric according objective.\n",
    "5. Define some baseline in terms of performance. Are there any minimum requirements?\n",
    "6. Can you rely (at any level) on human expertise? \n",
    "\n",
    "---\n",
    "\n",
    "The main goal of this project is to solve the following task:\n",
    "    \n",
    "> __Problem Statement__: Given information about people who unfortunately were embarked on the titanic, deciding weather or not\n",
    "  a person will survive shipwreck or not.\n",
    "  \n",
    " The solution provided here  is designed to be a stand-alone solution. Thus there is not any requirement about \n",
    " compatibility or integration with other models.\n",
    "\n",
    "It appears the best way to approach the problem is by using a supervised learning approach, i.e., by the means of \n",
    "some classification algorithm.\n",
    "\n",
    "Based on your these assumptions, you should choose a performance metric, such as \n",
    "accuracy, recall, precision. \n",
    "\n",
    "At this stage, it can be vague, you may reconsider this choice later in the project.\n",
    "\n",
    "\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "---\n",
    "__Suggestions__:\n",
    "\n",
    "1. Introduce the reader to the data. Provide a first qualitative analysis.\n",
    "2. Discuss where the data came from and how they have been collected.\n",
    "3. Check legal obligations, and get authorization if necessary. \n",
    "4. Convert the data to a format you can easily manipulate (without changing the\n",
    "data itself).\n",
    "5. Ensure sensitive information is deleted or protected (e.g., anonymized).\n",
    "6. Check the size and type of data (time series, sample, geographical, etc.).\n",
    "7. Sample a test set, put it aside, and never look at it (no data snooping!).\n",
    "\n",
    "---\n",
    "The dataset contains information like gender, ticket class, age, etc. related to the titanic passenger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed description about the meaning of each attribute and its values:\n",
    "\n",
    "* PassengerID - A column added by Kaggle to identify each row and make submissions easier\n",
    "* Survived - Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes) __(*)__\n",
    "* Pclass - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n",
    "* Sex - The passenger's sex\n",
    "* Age - The passenger's age in years\n",
    "* SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n",
    "* Parch - The number of parents or children the passenger had aboard the Titanic\n",
    "* Ticket - The passenger's ticket number\n",
    "* Fare - The fare the passenger paid\n",
    "* Cabin - The passenger's cabin number\n",
    "* Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "\n",
    "---\n",
    "\n",
    "__(*)__ this is thea attribute containing the class we aim to predict\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "---\n",
    "__Suggestions__:\n",
    "\n",
    "1. Create a copy of the data for exploration. If the dataset is too large, try  to focus a subset.\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration.\n",
    "3. Study each attribute and its characteristics like:\n",
    "    * Name, Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "    * Percentage of missing values\n",
    "    * Noisiness and type of noise 8tochastic, outliers, rounding errors, etc.)\n",
    "    * Type of distribution, e.g., Gaussian, uniform, logarithmic, etc.\n",
    "4. For supervised learning tasks, identify the target attribute(s).\n",
    "5. Visualize the data.\n",
    "6. Study the correlations between attributes.\n",
    "7. Identify the promising transformations you may want to apply.\n",
    "8. Identify extra data that would be useful.\n",
    "10. Document the insights deriving from this first exploration of the data.\n",
    "\n",
    "---\n",
    "\n",
    "Since the training set has a rather small size we can explore the data considering the entire dataset.\n",
    "The dataset dimensions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the presence of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the fraction of missing value for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.isnull().sum()/df.shape[0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Age`` and ``Cabin`` have missing values.\n",
    "\n",
    "Let's take a quick look at the numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The main goal of this stage is to understand what variables are important for predicting wether or not \n",
    "a person will survive the shipwreck.\n",
    "\n",
    "Two attributes that may be very promising are the age and the class of the ticket. \n",
    "\n",
    "Remember to check every assumptions.\n",
    "\n",
    "__Assumption 1__\n",
    "\n",
    "Male persons have lower probability of survival, as \n",
    " children and women have a higher priority.\n",
    " \n",
    "Verify the assumption!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_sex = df.groupby(['Sex', 'Survived'])[['PassengerId']].count()\n",
    "df_sex = df_sex.rename({'PassengerId': 'count'}, axis=1)\n",
    "df_sex['prob'] = df_sex['count'] / df.groupby('Sex').count()['Survived']\n",
    "df_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, one may expects that passengers with higher ticket class would have more privilege than low class passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_pclass = df.groupby(['Pclass', 'Survived'])[['PassengerId']].count()\n",
    "df_pclass = df_pclass.rename({'PassengerId': 'count'}, axis=1)\n",
    "df_pclass['prob'] = df_pclass['count'] / df.groupby('Pclass').count()['Survived']\n",
    "df_pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar analysis for the age attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Age`` is a real-valued attribute. \n",
    "\n",
    "Therefore it is better to plot the age distribution wrt to \n",
    "the ``Survived`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "survived = df[df[\"Survived\"] == 1]\n",
    "died = df[df[\"Survived\"] == 0]\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].hist(survived[\"Age\"].dropna(), alpha=0.5, color='red', bins=30)\n",
    "axes[1].hist(died[\"Age\"].dropna() ,alpha=0.5,color='blue',bins=30)\n",
    "axes[0].set_title('Survived')\n",
    "axes[1].set_title('Not Survived')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[1].set_xlabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that only a small fraction of people of age within 0 and 20 does not survive the shipwreck.\n",
    "\n",
    "In order to gather some other insight we plot the same histograms, but this time separating men from women.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "survived = df[df[\"Survived\"] == 1]\n",
    "died = df[df[\"Survived\"] == 0]\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].hist(survived.loc[survived[\"Sex\"] == \"male\"][\"Age\"].dropna(), label='Male', alpha=0.5, bins=30)\n",
    "axes[0].hist(survived.loc[survived[\"Sex\"] == \"female\"][\"Age\"].dropna(), label='Female', alpha=0.5, bins=30)\n",
    "\n",
    "axes[1].hist(died.loc[died[\"Sex\"] == \"male\"][\"Age\"].dropna(), label='Male', alpha=0.5, bins=30)\n",
    "axes[1].hist(died.loc[died[\"Sex\"] == \"female\"][\"Age\"].dropna(), label='Female', alpha=0.5, bins=30)\n",
    "\n",
    "axes[0].set_title('Survived')\n",
    "axes[1].set_title('Not Survived')\n",
    "\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[1].set_xlabel('Age')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that people with age within the interval 20, 40 are doomed, especially \n",
    "if they are male.\n",
    "\n",
    "\n",
    "Let's focus  ``SibSp``, since it is most likley that male people of that age were travelling with some relatives\n",
    "(the number of siblings or spouses the passenger had aboard the Titanic).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_sib = pd.pivot_table(df, index='SibSp', values='Survived')\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.plot(df_sib, marker='o')\n",
    "\n",
    "ax.set_xlabel(\"SibSp\")\n",
    "ax.set_title('Prob. of Survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As partially unveiled by the previous analysis, when the number the of siblings increase the probability of\n",
    "survival tends to decrease.\n",
    "\n",
    "Finally, since there may be some hidden pattern that we might have missed, we plot a factor matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_ = sns.pairplot(df,\n",
    "                 x_vars = ['Parch', 'Fare', 'SibSp'],\n",
    "                 y_vars = ['Parch', 'Fare', 'SibSp'],\n",
    "                 hue='Survived', \n",
    "                 diag_kind='kde', \n",
    "                 diag_kws={'alpha':0.5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "__Suggestion__\n",
    "Conclude this stage with  a section summarizing the major findings of \n",
    "the data exploration.\n",
    "\n",
    "---\n",
    "\n",
    "- Women are most likely to survive\n",
    "- The ticket values affects the probability of survival\n",
    "- Age of the passengers is an important feature. In some age-intervals the  death toll is significantly high.\n",
    "- In general, youngsters have a higher probability of survival.\n",
    "- There is a sort of correlation between the probability of survival and the number of relatives a person is travelling with.\n",
    "\n",
    "\n",
    "These findings should drive the preprocessing stage.\n",
    "\n",
    "For instance:\n",
    "* There are missing values in the ``Age`` and ``Cabin`` column\n",
    "* The ``Age`` column is real-valued attributes. We should discretize it.\n",
    "* There are several columns with categorical attributes, we may want to represent them with one-hot-encoding\n",
    "\n",
    "---\n",
    "## Prepare the Data\n",
    "---\n",
    "__Suggestions__\n",
    "\n",
    "1. Write functions for all data transformations you apply, for five reasons:\n",
    "    * So you can easily prepare the data the next time you get a fresh dataset\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "    * To clean and prepare new data instances once your solution is live\n",
    "    * To make it easy to treat your preparation choices as hyperparameters\n",
    "2. Data cleaning:\n",
    "    * Fix or remove outliers \n",
    "    * Fill in missing values (e.g., with zero, mean, median...) or drop their rows (or columns).\n",
    "3. Feature selection:\n",
    "    * Drop the attributes that provide no useful information for the task.\n",
    "4. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features.\n",
    "    * Decompose features (e.g., categorical, date/time, etc.).\n",
    "    * Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\n",
    "    * Aggregate features into promising new features.\n",
    "\n",
    "5. Feature scaling: standardize or normalize features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Dealing with ``Cabin`` and ``Embarked``__\n",
    "\n",
    "Since ``Cabin`` has 77% of missing values, drop it.\n",
    "\n",
    "Analogously we can drop the few NaN instances in the corresponding to the ``Embarked`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('Cabin', axis=1)\n",
    "df = df.dropna(subset=['Embarked'], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``Age``__\n",
    "\n",
    "It is better to discretize this column, as we have seen that there are age-range \n",
    "most significant the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('discretize', KBinsDiscretizer(n_bins=5, encode='onehot'))\n",
    "    ])\n",
    "\n",
    "age_one_hot = age_pipeline.fit_transform(df[['Age']])\n",
    "age_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``PClass``__\n",
    "\n",
    "The best way of representing this attribute would be by a one-hot-encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pclass_pipeline = Pipeline([\n",
    "    ('discretize', OneHotEncoder())\n",
    "])\n",
    "\n",
    "pclass_one_hot = pclass_pipeline.fit_transform(df[['Pclass']])\n",
    "pclass_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Dealing with ``Name``, ``Sex``, ``Ticket``, ``Embarked``__\n",
    "\n",
    "These columns are categorical. It  is better to have them as a one-hot-encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "cat_df = df.select_dtypes(['object'])\n",
    "cat_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the ``Ticket`` column does not have any value, we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('Ticket', axis=1)\n",
    "cat_df = cat_df.drop('Ticket', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Name`` column can bring some interesting information. \n",
    "\n",
    "In fact, Mr., Miss, Mrs., Maester, can denote the social class of an individual. This may have some degree of influence on\n",
    "the probaiblity of survival.\n",
    "\n",
    "Therefore, we will replace the original column with another one containing only the tile of a person (e.g., Mr., Mrs...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "names = cat_df.Name.str.split(' ')\n",
    "lnames = []\n",
    "for entry in names:\n",
    "    lnames.extend(entry)\n",
    "c = Counter(lnames)\n",
    "print(c.most_common()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common titles seems to be: Mr., Miss., and Master.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a pipeline for transforming every categorical columns into a one-hot-encoding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cat_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no more missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(\n",
    "    [('onehot', OneHotEncoder())]\n",
    ")\n",
    "sex_tick_emb_one_hot = cat_pipeline.fit_transform(cat_df)\n",
    "print(cat_df.columns,sex_tick_emb_one_hot.shape, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Finally, we can merge all these transformations into a single pipeline and then obtain the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Xdf, ydf = df.loc[:, [\"Name\", \"Sex\", \"Embarked\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\"]], df.loc[:,[\"Survived\"]]\n",
    "# replace the name column\n",
    "Xdf['Name'] = cat_df['Name']\n",
    "Xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cat_attribs = list(cat_df.columns)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "        (\"age_pipe\", age_pipeline, ['Age']),\n",
    "        (\"pclass_pipe\", pclass_pipeline, [\"Pclass\"])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "X, y = full_pipeline.fit_transform(Xdf), ydf.values.reshape(-1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Short-List Promising Models\n",
    "---\n",
    "__Suggestions__:\n",
    "\n",
    "1. If the data is huge, you may want to sample smaller training sets so you can train\n",
    "many different models in a reasonable time (be aware that this penalizes complex\n",
    "models such as large neural nets or Random Forests).\n",
    "Once again, try to automate these steps as much as possible.\n",
    "2. Train many quick and dirty models from different categories (e.g., linear, naive\n",
    "   Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "3. Measure and compare their performance. \n",
    "   For each model, use N-fold cross-validation and compute the mean and stan‐\n",
    "   dard deviation of the performance measure on the N folds.\n",
    "4. Analyze the most significant variables for each algorithm.\n",
    "5. Analyze the types of errors the models make.\n",
    "   What data would a human have used to avoid these errors?\n",
    "6. Have a quick round of feature selection and engineering.\n",
    "7. Have one or two more quick iterations of the five previous steps.\n",
    "8. Short-list the top three to five most promising models, preferring models that\n",
    "   make different types of errors.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compute_performance(estimators, X, y, scoring_funs, cross_val=True):\n",
    "    '''\n",
    "    Compute every score function for each estimator\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        estimators : a list of predictors\n",
    "        X : data used for computed the corss_validation_test\n",
    "        y : target values\n",
    "        scoring_fun : list of performances measures\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        A dictionary containing the score of each estimator\n",
    "    '''\n",
    "    score_dict = {}\n",
    "    for e in estimators:\n",
    "        score_dict[e.__class__.__name__] = {}\n",
    "        if not cross_val:\n",
    "            # we call predict on the estimator\n",
    "            y_pred = e.predict(X)\n",
    "\n",
    "        for f in scoring_funs: \n",
    "            if cross_val:\n",
    "                score = cross_val_score(e, X ,y, cv=10 ,scoring=\"accuracy\").mean()\n",
    "                name = f\n",
    "            else:\n",
    "                _f, name = f\n",
    "                score = _f(y_pred, y)\n",
    "            score_dict[e.__class__.__name__][name] = score\n",
    "    \n",
    "    return score_dict\n",
    "\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we will try several classification algorithms.\n",
    "\n",
    "__SGDClassifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(max_iter=5, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "compute_performance([sgd_clf], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NaiveBayes__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf = nb_clf.fit(X_train.toarray(), y_train)\n",
    "compute_performance([nb_clf], X_train.toarray(), y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LogisticRegression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_clf = LogisticRegression(solver='lbfgs')\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "compute_performance([logreg_clf], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Support Vector Machine__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "compute_performance([svc], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Fine-Tune the System__\n",
    "\n",
    "Notes:\n",
    "1. You may want to use as much data as possible for this step, especially as you move\n",
    "  toward the end of fine-tuning.\n",
    "2. Treat your data transformation choices as hyperparameters, especially when\n",
    "you are not sure about them (e.g., should I replace missing values with zero or\n",
    "with the median value? Or just drop the rows?).\n",
    "2. Unless there are very few hyperparameter values to explore, prefer random\n",
    "search over grid search. \n",
    "3. Try Ensemble methods. Combining your best models will often perform better\n",
    "than running them individually.\n",
    "4. Once you are confident about your final model, measure its performance on the\n",
    "   test set to estimate the generalization error.\n",
    "---\n",
    "\n",
    "It appears that the last two models are the most promising ones.\n",
    "\n",
    "For this reason in the following section we are going to fine-tuning their parameter in\n",
    "order to conduct a more involved analysis.\n",
    "\n",
    "\n",
    "__Parameter Tuning for the LogisticRegression__\n",
    "\n",
    "We decide to tune the some of its parameter. More specifically: the ``solver`` and the ``penalty`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_logreg_grid = [\n",
    "    {'penalty': ['l2'], 'solver': ['newton-cg', 'sag','lbfgs']},\n",
    "  ]\n",
    "\n",
    "grid_search = GridSearchCV(logreg_clf, param_logreg_grid, cv=10,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Param:\", grid_search.best_params_)\n",
    "best_logreg_clf = grid_search.best_estimator_\n",
    "compute_performance([best_logreg_clf], X_train, y_train, scoring_funs=['accuracy', 'recall','precision','f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Parameter Tuning for the SVC_\n",
    "\n",
    "Try different kernel function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "param_svc_grid = [\n",
    "    {'kernel': ['linear', 'poly', 'rbf']},\n",
    "  ]\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_svc_grid, cv=10,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Param:\", grid_search.best_params_)\n",
    "best_svc = grid_search.best_estimator_\n",
    "compute_performance([best_svc], X_train, y_train, scoring_funs=['accuracy', 'recall','precision','f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "From this analysis, it appears that the SVC is the best choice. \n",
    "\n",
    "Summarize results into a table \n",
    "\n",
    "## Ensemble Learning\n",
    "\n",
    "Before the ultimate decision it is better to try some ensemble.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "                SVC(kernel='rbf'), #its best configuration \n",
    "                n_estimators=50,\n",
    "                bootstrap=True,\n",
    "                n_jobs=-1,\n",
    "                oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "compute_performance([bag_clf], X_train, y_train, scoring_funs=['accuracy', 'recall','precision','f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the BaggingClassifier is not able to boost the performance of the single learner.\n",
    "\n",
    "Let's try with other ensamble methods.\n",
    "\n",
    "*Voting Classifier*\n",
    "\n",
    "Since LogisticRegression and SVC classifiers share similar performance, we can try to combine them\n",
    "in a VotingClassifier. \n",
    "\n",
    "We also include an additional DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "best_svc.probability = True\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', best_logreg_clf), ('svc', best_svc), ('dct', DecisionTreeClassifier())],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "compute_performance([voting_clf], X_train, y_train, scoring_funs=['accuracy', 'recall', 'precision', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are able to get a slightly boost in terms of accuracy and f1 score.\n",
    "\n",
    "*Random Forest*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(X,y)\n",
    "compute_performance([rnd_clf], X_train, y_train, scoring_funs=['accuracy', 'recall', 'precision', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest seems to be worse than the other models.\n",
    "However we can use it for computing the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i,x in enumerate(rnd_clf.feature_importances_):\n",
    "    print(\"Position:{}: {}\".format(i,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "__Suggestion__:\n",
    "For a more exhaustive evaluation, you should try other ensamble learning methods (see Lecture 7a).\n",
    "However, if you get to this point, where none ensamble methods seems to guarantee a considerable boost\n",
    "in performance, you have basically two choices:\n",
    "\n",
    "   > (i) You settle for the best performance recorded so far, but you should also motivate why you were unable to get any boost in terms of performance\n",
    "\n",
    "   > (ii) You can reconsider some decisions taken in the earlier steps.\n",
    "    For instance, we neither normalized or standardize our dataset. \n",
    "    Another option would be to sample a subset of attributes from the original data, driven by the feature importance\n",
    "    compute by the RandomForestClassifier (be careful with the actual attributes they refer to, especially with One-Hot-Encoding\n",
    "\n",
    "Now, since I am the examiner and not the subject being examined, we will pretend the BaggingClassifier did an excellent job! :-)\n",
    "\n",
    "---\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "Based on the previous analysis, three models have emerged as the most promising ones.\n",
    "\n",
    "In this section we you test their performance against the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score_dict = compute_performance([rnd_clf, voting_clf,bag_clf], X_test, y_test, scoring_funs=[(accuracy_score, \"accuracy\"), (f1_score, \"f1\")], cross_val=False)\n",
    "score_df = pd.DataFrame(score_dict)\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Present the results and take the final decision about what is the best model.\n",
    "\n",
    "---\n",
    "__Suggestions__\n",
    "\n",
    "Here is a list of suggestions for improving the quality of this analysis.\n",
    "\n",
    "* You may want to remove some features according to the random forest features importance\n",
    "* You may want to show the performance of each classifier wrt to other measure (e.g., the ROC curve)\n",
    "* If at the first try you don't succeed you can also report the failure and try to explain why that happens. \n",
    "* Note also that the ``train_test_split`` should be called earlier than what we did. Also, it is better use pipeline for \n",
    "  automating the training episodes and the dataset transformation.\n",
    "\n",
    "---\n",
    "## Present Your Solution\n",
    "1. Document what you have done.\n",
    "2. Create a nice presentation.\n",
    "   Make sure you highlight the big picture first.\n",
    "3. Explain why your solution achieves the business objective.\n",
    "4. Don’t forget to present interesting points you noticed along the way. Describe what worked and what did not. List your assumptions and your system’s limitations.\n",
    "5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one    predictor of housing prices”\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "name": "Lecture 8.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
