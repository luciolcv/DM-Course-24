{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining, Lecture 1\n",
    "\n",
    "## Handling data with Pandas\n",
    "\n",
    "pandas contains high-level data structures and manipulation tools designed to make data\n",
    "analysis fast and easy in Python. \n",
    "\n",
    "pandas is built on top of NumPy and makes it easy to\n",
    "use in NumPy-oriented applications.\n",
    "\n",
    "Some of the major features of pandas are:\n",
    "\n",
    "* Data structures with labeled axes supporting automatic or explicit data alignment.\n",
    "  This prevents common errors resulting from misaligned data and working with\n",
    "  differently-indexed data coming from different sources.\n",
    "* Integrated time series functionality\n",
    "* The same data structures handle both time series data and non-time series data.\n",
    "* Arithmetic operations and reductions (like summing across an axis) would pass\n",
    "  on the metadata (axis labels)\n",
    "* Flexible handling of missing data.\n",
    "* Merge and other relational operations found in popular database databases (SQL-\n",
    "  based, for example).\n",
    "---\n",
    "\n",
    "Let's start and import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "from io import StringIO \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "IMG_PATH, DATA_PATH = \"./img\", \"./data\"\n",
    "\n",
    "\n",
    "def describe(a):\n",
    "    if type(a) is np.ndarray:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ndtype:{}\\ntype: {}\".format(a, a.shape, a.dtype, type(a)))\n",
    "    elif type(a) is pd.Series:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ndtype:{}\\nname:{}\\nindex-name:{}\\nindex-type:{}\\ntype:{}\".format(a, a.shape, a.dtype, a.name, a.index.name,type(a.index), type(a)))\n",
    "    elif type(a) is pd.DataFrame:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ntype:{}\".format(a, a.shape,type(a)))\n",
    "    else:\n",
    "        print(\"{}, type:{}\".format(a, type(a)))\n",
    "\n",
    "\n",
    "hrule = lambda x : \"=\"*x\n",
    "Hrule = lambda x,y: \"=\"*(x//2)+y+\"=\"*(x//2)\n",
    "Data = lambda file : os.path.join(DATA_PATH, file)\n",
    "Img  = lambda img : os.path.join(IMG_PATH, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to pandas Data Structures\n",
    "\n",
    "In order to get started with pandas, you need to get comfortable with its two workhorse\n",
    "data structures: Series and DataFrame.\n",
    "\n",
    "### Series\n",
    "A Series is a one-dimensional array-like object containing an array of data (of any\n",
    "NumPy data type) and an associated array of data labels, called its index. \n",
    "\n",
    "The simplest example of pandas Series is just a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series(np.arange(10))\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: Likewise vanilla numpy array, a Series object has also a dtype and a shape.\n",
    "\n",
    "---\n",
    "\n",
    "Please note the main difference with numpy arrays. A Series object is \n",
    "associated with an object of type index (printed  on the left side).\n",
    "\n",
    "\n",
    "Since in the previous case we did not specify any index, by default pandas assign\n",
    "all the integers within the interval $[0, N)$, where $N$ is the length of the series.\n",
    "\n",
    "We can also access to index ad values of a Series object, independently from each other, as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(obj.index, obj.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to create a Series with an index associated with each data point.\n",
    "\n",
    "You can specify an index while instantiating the Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series([1,2,3,4], index=list('abcd'))\n",
    "describe(obj)\n",
    "print(obj['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use values of the index when selecting\n",
    "single values or a set of values. \n",
    "\n",
    "Likewise NumPy, a Series object accept all the indexing strategies\n",
    "provided by a regular array.\n",
    "\n",
    "---\n",
    "\n",
    "**Example** - Get Familiar with Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A new Series from obj.\n",
    "    Result\n",
    "    -----\n",
    "    values = [4,4,4,7,3]\n",
    "\n",
    "Hint: Fancy indexing\n",
    "''' \n",
    "print(obj2[['d']*3 + ['b', 'c']])\n",
    "print(hrule(20))\n",
    "      \n",
    "'''\n",
    "A series containing only the\n",
    "positive values\n",
    "    Result\n",
    "    -----\n",
    "    values = [4,7,3]\n",
    "''' \n",
    "print(obj2[obj2>0])\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "    Result\n",
    "    ------\n",
    "    values = [16, 49, 25, 9]\n",
    "'''\n",
    "print(obj2**2)\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It should be noted that a Series is very similar to a fixed-length, ordered dict.\n",
    "In fact a Series is also a mapping of index to data values. \n",
    "\n",
    "It can be therefore replaced into many functions that expect a\n",
    "dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "obj = Series(\n",
    "    [1,2,3,4,5,6,7],\n",
    "    index=np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe']),\n",
    ")\n",
    "\n",
    "# Note: 1 key multiple values\n",
    "print(obj['Joe']) # access like a dict \n",
    "print(hrule(20))\n",
    "\n",
    "print(obj.Joe)  # access as an object field\n",
    "print(hrule(20))\n",
    "\n",
    "# test membership\n",
    "print('Joe' in obj)\n",
    "print('Frank' in obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the index is not unique. Several values can be associated with the same index\n",
    "\n",
    "---\n",
    "It is also straightforward to construct a Series from a Python dict.\n",
    "You just need to give the constructor a dict  as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "obj3 = Series(sdata)\n",
    "print(obj3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is ordered by default. However we can also specify the exact order we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "states = ['California', 'Ohio', 'Oregon', 'Texas']\n",
    "\n",
    "obj = Series(sdata, index=states) \n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: California is not included in the original dict sdata. For this reason in the Series object appears a NaN corresponding to the *California* index\n",
    "\n",
    "---\n",
    "We can obtain a mask for selecting all the (not)null values from the Series as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(obj.isnull()) # equivalently pd.isnull(obj)\n",
    "print(hrule(20))\n",
    "\n",
    "describe(pd.notnull(obj)) # equivalently obj.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Series provide also the user with the ability of specifying a name for the entire Series object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj.name = \"population\"\n",
    "obj.index.name = \"states\"\n",
    "describe(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series’s index can be altered in place by assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series(\n",
    "    [1,2,3,4,5,6,7],\n",
    "    index=np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe']),\n",
    ")\n",
    "obj.index = ['Joe', 'Bob', 'Will', 'Bob', 'Will', 'Joe', 'Joe'] \n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "A DataFrame represents a tabular, spreadsheet-like data structure containing an ordered\n",
    "collection of columns, each of which can be a different value type (numeric,\n",
    "string, boolean, etc.). \n",
    "\n",
    "A DataFrame has both a row and column index; it can be\n",
    "seen as a dict of Series sharing the same index.\n",
    "\n",
    "---\n",
    "\n",
    "There is a number of ways to construct a DataFrame. \n",
    "\n",
    "Nonetheless, one of the most common ways is to build it starting\n",
    "from a dict of equal-length lists or NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# each key will correspond to a column of the DataFrame\n",
    "data = {\n",
    "    'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n",
    "    'year': [2000, 2001, 2002, 2001, 2002],\n",
    "    'pop': [1.5, 1.7, 3.6, 2.4, 2.9]\n",
    "}\n",
    "df = DataFrame(data)  # Note: the index is automatically assigned \n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rearrange the columns of the dataframe by explicitly defining the exact order in which you want them to appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise the Series, passing a column that isn’t contained in data will lead to a column of NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(df, columns=['year', 'state', 'pop', 'dept'])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a DataFrame\n",
    "\n",
    "### Accessing the column\n",
    "You can directly access to a column of the DataFrame using a dict-like or attribute-like notation. \n",
    "A column of the dataframe  is returned as a Series object having the same index as the original dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(df.state)\n",
    "print(hrule(20))\n",
    "describe(df['state']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column indexing can also be used for adding a new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['density'] = 10 # broadcast a scalar value\n",
    "describe(df)\n",
    "print(hrule(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['dept'] = np.arange(df.shape[0]) # we can also use a ndarray\n",
    "describe(df)\n",
    "print(hrule(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['pil'] = Series(np.arange(df.shape[0])) # Pay Attention\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to create a column from either a list or a numpy array, you must ensure that they have the same length.\n",
    "\n",
    "If you create a column from a Series, in addition to the length of the series, you must ensure that the Series and\n",
    "the DataFrame share the same index (on the rows of the DataFrame).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise** - Look at those NAN. Replace it so that half the entries take the value ``dept1`` and the the remaining \n",
    "entries get ``dept2``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rows, _ = df.shape\n",
    "firstHalf, secondHalf = int(np.floor(rows/2)), int(np.ceil(rows/2))\n",
    "tmp = Series(['dept1']*firstHalf+['dept2']*secondHalf, df.index)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['dept'] = tmp\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Remember that the Series object returned by ``df[columnName]`` is always *view* on the original data, thus any change to that object will be reflected\n",
    "on the dataframe (unless we call copy() the Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['state'][0] = 'New York'\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Wait a minute! What the heck is this Warning?**\n",
    "\n",
    "First of all, it is just a warning and not an error.\n",
    "\n",
    "While an error indicates that something is broken, \n",
    "such as invalid syntax or an attempt to reference an undefined variable, \n",
    "the job of a warning is to alert the programmer about potential bugs or issues with their program,\n",
    "despite the permitted operation.\n",
    "\n",
    "\n",
    "*SettingWithCopyWarning* informs you that\n",
    "your operation might not have worked as expected and that you \n",
    "should check the result to make sure you haven’t made \n",
    "a mistake.\n",
    "\n",
    "To understand what SettingWithCopyWarning is about, \n",
    "it’s helpful to understand that some actions in pandas can return a view of your data, \n",
    "and others will return a copy.\n",
    "\n",
    "Therefore this Warning is kindly suggesting to check whether\n",
    "the instruction you issued has worked as you expected to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Image(os.path.join(IMG_PATH, 'viewvscopy.png'), width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Accessing via iloc vs. loc\n",
    "Let's create another DataFrame and try to access to the first row with a dict-like notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'], index=np.flip(np.arange(0, len(next(iter(data)))))) # LOOK: the index is in reverse order\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[0] #ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following instructions are available to access a row of the dataframe.\n",
    "\n",
    "**iloc**\n",
    "\n",
    "iloc gets rows (or columns) at particular positions in the index (so it takes integers or boolean arrays)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'], index=np.flip(np.arange(0, len(next(iter(data)))))) # LOOK: the index is in reverse order\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"index: {}\".format(df.index))\n",
    "\n",
    "\n",
    "print(Hrule(20, \"First Element\"))\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Second Element\"))\n",
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Element with index 4\"))\n",
    "print(df.loc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"state value of element with index 3\"))\n",
    "print(df.iloc[1]['state'])\n",
    "print(hrule(5))\n",
    "print(df.iloc[1, 1]) # 1 because 'state' is the column with index 1\n",
    "print(df.iloc[1].state) # accessing a field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loc**\n",
    "\n",
    "gets rows (or columns) with particular labels from the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_ = DataFrame(data, columns=['year', 'state', 'pop'], index=[chr(x) for x in range(97, 97+len(next(iter(data))))]) \n",
    "describe(df_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Element with index a\"))\n",
    "print(df_.loc['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Element with index b, pop\"))\n",
    "print(df_.loc['a']['pop']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(hrule(5))\n",
    "print(df_.loc['a', 'pop']) # the same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Exercise**\n",
    "Get familiar with dataframe indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'], index=[chr(x) for x in range(97, 97+len(next(iter(data))))])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "    a    2000\n",
    "    b    2001\n",
    "    c    2002\n",
    "    d    2001\n",
    "    e    2002\n",
    "Requirement: Dict-like access\n",
    "''' \n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "-----\n",
    "    'Ohio' of the second element\n",
    "Requirement: Object-like access + dict-like access\n",
    "''' \n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "    year       2000\n",
    "    state      Ohio\n",
    "    pop         1.5\n",
    "    density       0\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "    year    2000\n",
    "    pop      1.5\n",
    "    Name: a\n",
    "Hint: Fancy Indexing\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "  array([2001, 'Nevada', 2.4], dtype=object)\n",
    "Hint: .values\n",
    "'''\n",
    "describe(\n",
    "## your code here \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "     array([2001, 2.4], dtype=object) (the fourth element)\n",
    "Hint: .values\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "    array([[2000, 'Ohio', 1.5],\n",
    "       [2001, 'Ohio', 1.7],\n",
    "       [2002, 'Ohio', 3.6],\n",
    "       [2001, 'Nevada', 2.4],\n",
    "       [2002, 'Nevada', 2.9]], dtype=object)\n",
    "\n",
    "The entire dataframe as a ndarray\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Objects\n",
    "The index of a dataframe is responsible for holding the axis labels and other\n",
    "metadata (e.g., axis name).\n",
    "\n",
    "Any array or other sequence-like object of labels can serve \n",
    "as an index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series(range(3), index=['a', 'b', 'c'])\n",
    "index = obj.index\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index objects are immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index[1] = 'c' # ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataframe axes has its own index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df.index) # row index\n",
    "print(df.columns) # column index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following table contains the Main Index objects in pandas\n",
    "\n",
    "|Class|Description|\n",
    "|:-:|:-:|\n",
    "|Index |The most general Index object, representing axis labels in a NumPy array of Python objects. |\n",
    "|Int64Index | Specialized Index for integer values. |\n",
    "|MultiIndex | “Hierarchical” index object representing multiple levels of indexing on a single axis. Can be thought of as similar to an array of tuples.|\n",
    "|DatetimeIndex | Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).|\n",
    "|PeriodIndex | Specialized Index for Period data (timespans).|\n",
    "\n",
    "\n",
    "The following table contains a list useful index methods\n",
    "\n",
    "|Method| Description|\n",
    "|:-:|:-:|\n",
    "|append | Concatenate with additional Index objects, producing a new Index|\n",
    "|diff |Compute set difference as an Index|\n",
    "|intersection |Compute set intersection|\n",
    "|union |Compute set union|\n",
    "|isin |Compute boolean array indicating whether each value is contained in the passed collection|\n",
    "|delete| Compute new Index with element at index i deleted|\n",
    "|drop |Compute new index by deleting passed values|\n",
    "|insert| Compute new Index by inserting element at index i|\n",
    "|is_monotonic| Returns True if each element is greater than or equal to the previous element|\n",
    "|is_unique| Returns True if the Index has no duplicate values|\n",
    "|unique |Compute the array of unique values in the Index|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Functionality\n",
    "Here we discuss a series of fundamental instructions for interacting with\n",
    "the data contained in either a DataFrame or a Series.\n",
    "\n",
    "### Re-indexing\n",
    "Create a new object where data\n",
    "with the data conformed to a new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj2 = obj.reindex(['a','a', 'b','b', 'd','c','d'])\n",
    "describe(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the original object are arranged  according to the list passed as input to ``reindex``.\n",
    "\n",
    "We can also insert new element via the new-index.\n",
    "More precisely, for each entry whose corresponding index is not\n",
    "already included in the original frame pandas will create a new entry\n",
    "with a given default value, which is ``NaN`` if it is not provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj2 = obj.reindex(['a', 'z']) # NaN as default behavior\n",
    "describe(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj2 = obj.reindex(['a', 'z'], fill_value = -1) # full with -1\n",
    "describe(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``reindex`` can be also applied upon DataFrame. In this case\n",
    "it returns a *copy* of the original frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Re-indexing rows\"))\n",
    "df_ = df.reindex(['a', 'b', 'c', 'd']) # re-indexing rows\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Re-indexing columns\"))\n",
    "df_ = df.reindex(columns=['Texas', 'Ohio', 'Ohio'])\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Re-indexing rows & columns\"))\n",
    "df_ = df.reindex(index=['a','a','b'], columns=['Texas', 'Ohio', 'Ohio'])\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping entries from an axis\n",
    "Dropping means removing an entry from the dataframe along a certain axis.\n",
    "\n",
    "* Dropping along axis 1 means removing a column of the dataframe.\n",
    "\n",
    "* Dropping along axis 0 means removing an entry of the dataframe.\n",
    "\n",
    "It can be done via the ``drop`` method, which asks for an index \n",
    "of the dataframe, i.e., the index to be removed.\n",
    "\n",
    "**Note**: It always returns a copy of the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(np.arange(16).reshape((4, 4)),\n",
    "                index=['Ohio', 'Colorado', 'Utah', 'New York'],\n",
    "                columns=['one', 'two', 'three', 'four'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_ = df.drop(['Colorado', 'Utah']) #dropping rows - by default axis=0\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df\n",
    "df_test.drop(['Colorado'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_ = df.drop(['one','four'], axis=1) # dropping columns\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, selection, and filtering\n",
    "\n",
    "Indexing is very similar to numpy. \n",
    "Every indexing strategy we have seen for numpy also applies in this context.\n",
    "\n",
    "---\n",
    "**Exercise**: Adopt and observe how the indexing techniques of numpy ndarray apply to Series and DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series(np.arange(4.), index=reversed(['a', 'b', 'c', 'd']))\n",
    "df =DataFrame(np.arange(16).reshape((4, 4)),\n",
    "              index=['Ohio', 'Colorado', 'Utah', 'New York'],\n",
    "              columns=['one', 'two', 'three', 'four'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Series\"))\n",
    "describe(obj)\n",
    "\n",
    "print(Hrule(20, \"DataFrame\"))\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The first element of the series (positional indexing)\n",
    "Result\n",
    "------\n",
    "    0\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The d-element of the series\n",
    "Result\n",
    "-----\n",
    "    0\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "All the elements within d and c\n",
    "Result\n",
    "------\n",
    "    d 0\n",
    "    c 1\n",
    "Hint: slice operator    \n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Return all the elements less or equal to 2\n",
    "Result\n",
    "------\n",
    "    a    0.0\n",
    "    b    1.0\n",
    "''' \n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set all the elements between\n",
    "the firsts and the last-1 element\n",
    "of obj to -1\n",
    "'''\n",
    "obj_ = obj.copy()\n",
    "obj_[:-1] = -1\n",
    "\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select the first two rows\n",
    "of the dataframe\n",
    "'''\n",
    "describe(\n",
    "    df.iloc[:2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select all the element of\n",
    "the dataframe with value greater\n",
    "than 3 in column 'three'\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "All the columns except\n",
    "the last one\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "As before, all the rows corresponding to\n",
    "\"Utah\". All the columns within\n",
    "\"two\" and \"three\".\n",
    "\n",
    "Hint: use loc function\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Arithmetic and data alignment\n",
    "\n",
    "Every math based operation works on indexes. It means that pandas\n",
    "performs operations between entries having the same index.\n",
    "\n",
    "There is only one important aspect that you need to be aware of:\n",
    "the behavior of arithmetic between objects with different indexes. \n",
    "\n",
    "When adding together objects, if any index pairs are not\n",
    "the same, the index of the object obtained as the result of this operation will have the union of their\n",
    "indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(s1+s2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames have the same behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(df1 + df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``NaN`` values can be prevented by specifying a value with the ``fill_value`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fill the NaN corresponding to index mismatching\n",
    "tmp = df1.add(df2, fill_value=2, axis=1)\n",
    "describe(tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?df.add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fill the NaN corresponding to missing columns\n",
    "tmp_columns = df2.columns.union(df1.columns)   # set operations between index\n",
    "tmp = df1.reindex(columns=tmp_columns, fill_value=0)\n",
    "describe(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations between DataFrame and Series\n",
    "By default, arithmetic between DataFrame and Series matches the index of the Series\n",
    "on the DataFrame's columns, broadcasting down the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "series = frame.iloc[0]\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(frame)\n",
    "print(hrule(20))\n",
    "\n",
    "describe(series)\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(frame-series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function application and mapping\n",
    "\n",
    "NumPy ufuncs (element-wise array methods) work fine with pandas objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "describe(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(np.abs(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x :  x.max() - x.min()\n",
    "describe(frame.apply(f, axis=0)) # compute f over each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(frame.apply(f, axis=1)) # compute f over each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Many of the most common array statistics (like sum and mean ) are also ``DataFrame`` methods.\n",
    "Therefore, unless we want to compute some weird function on the dataframe, there is no need to pass\n",
    "the function via the ``apply`` method.\n",
    "\n",
    "The function passed to apply need not return a scalar value, it can also return a Series or a ndarray\n",
    "with multiple values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return Series([x.min(), x.max()], index=['min', 'max'])\n",
    "\n",
    "def f_(x):\n",
    "    return [x.min(), x.max()] # it is better to use a Series\n",
    "\n",
    "frame = DataFrame(np.random.randint(0,100,12).reshape(4,3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "describe(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(frame.apply(f))\n",
    "print(hrule(50))\n",
    "describe(frame.apply(f, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to axis-wise functions, in pandas we can also use\n",
    "element-wise function.\n",
    "\n",
    "Suppose you wanted to compute a\n",
    "formatted string from each floating point value in frame.\n",
    "You can do it with **map**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise**\n",
    "\n",
    "Get familiar with element-wise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pow2 = lambda x : x**2\n",
    "sqrt = lambda x : np.sqrt(x)\n",
    "'''\n",
    "Apply pow2 element-wise over frame\n",
    "'''\n",
    "describe(\n",
    "## your code here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add a column to frame whose values are\n",
    "the square root of the b column\n",
    "'''\n",
    "frame['new'] = sqrt(frame.b)\n",
    "describe(\n",
    "## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compute min, mean, std, max\n",
    "along axis 1 as a new data frame\n",
    "Hint: define a new function\n",
    "'''\n",
    "def statFunc(entry):\n",
    "    ## your code here\n",
    "\n",
    "df_ = frame.apply(statFunc, axis=1)\n",
    "describe(\n",
    "    df_\n",
    ")\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the elements greater than 50 to 0. \n",
    "Then replace all the zero valued elements in the ``b`` column to 'zero'.\n",
    "Hint: remember the ``np.where`` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def f50(x):\n",
    "    ## your code here\n",
    "\n",
    "frame_= frame.apply(f50)\n",
    "frame_['b'] = frame_.b.map({0:'B'}, na_action='ignore')\n",
    "\n",
    "describe(frame_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and ranking\n",
    "\n",
    "We can sort both the index or the values of a dataframe.\n",
    "\n",
    "When we call a sort-based method pandas always returns a copy of\n",
    "the original frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series(range(4), index=['d', 'a', 'b', 'c'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame = DataFrame(np.arange(8).reshape((2, 4)), \n",
    "                  index=['three', 'one'], columns=list(\"dabc\"))\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Sorting Series\"))\n",
    "sorted_obj = obj.sort_index() # sort_index returns a copy\n",
    "describe(sorted_obj) # lex-sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "With DataFrames we can choose the\n",
    "axis upon which performing the sorting operation\n",
    "'''\n",
    "print(Hrule(20, \"Sorting Data-Frames\"))\n",
    "sorted_frame_ax0 = frame.sort_index() # default axis=0\n",
    "describe(sorted_frame_ax0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sorted_frame_ax1 = frame.sort_index(axis=1, ascending=True)\n",
    "describe(sorted_frame_ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort a Series by its *values*, use the *sort_vales* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series([4,3,2,1,np.NaN], index=list(\"cdabs\"))\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "objA = obj.sort_values(ascending=False) # it always returns a copy\n",
    "objD = obj.sort_values()  # by default ascending=False\n",
    "describe(objA) \n",
    "print(hrule(20))\n",
    "describe(objD)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: NaNs are always pushed to the end of the Series, regardless of the order.\n",
    "\n",
    "----\n",
    "### Summarizing and Computing Descriptive Statistics\n",
    "pandas objects are equipped with a set of common mathematical and statistical methods.\n",
    "\n",
    "Most of these fall into the category of reductions or summary statistics, i.e., methods\n",
    "that extract a single value (like the sum or mean) from a Series or a compound of\n",
    "Series coming from either the rows or the columns of a DataFrame.\n",
    "\n",
    "The main difference with the NumPy array dedicated methods and the\n",
    "pandas counterparts is the ability to (automatically) handle missing data.\n",
    "\n",
    "Beside all the classic methods for computing statistics information\n",
    "along each axis of a datafame, there are two very useful methods that\n",
    "allows us to have a `first' approach with the data.\n",
    "\n",
    "These methods are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],\n",
    "                   [np.nan, np.nan], [0.75, -1.3]],\n",
    "                    index=['a', 'b', 'c', 'd'],\n",
    "                    columns=['one', 'two'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``describe`` method provides summary statistics all at once.\n",
    "The ``info`` method provides information about the type of data stored in the dataframe.\n",
    "In particular, it highlights the presence of missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Image(os.path.join(IMG_PATH, 'statistics.png'), width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values, Value Counts, and Membership\n",
    "Another class of related methods extracts information about the values contained in a\n",
    "one-dimensional Series. To illustrate these, consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\n",
    "unique_values = obj.unique() # return all distinct values as a ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# return a Series containing counts of unique values\n",
    "value_count = obj.value_counts(sort=True) \n",
    "describe(value_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique values are not necessarily returned in sorted order, but could be sorted\n",
    "after the fact if needed ( uniques.sort() )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unique_values.sort()\n",
    "describe(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table describes several methods related\n",
    "to sets operation with Series\n",
    "\n",
    "|Method|Description|\n",
    "|--|--|\n",
    "| isin |Compute boolean array indicating whether each Series value is contained in the passed sequence of values |\n",
    "| match | Compute integer indices for each value in an array into another array of distinct values; helpful for data alignment and join-type operation |\n",
    "| unique | Compute array of unique values in a Series, returned in the order observed |\n",
    "| value_counts | Return a Series containing unique values as its index and frequencies as its values, ordered count in descending order |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Storage, and File Formats\n",
    "\n",
    "pandas features a number of functions for reading tabular data as a DataFrame\n",
    "object. The table below summarizes some of them, though read_csv and read_table are\n",
    "likely the ones you’ll use the most.\n",
    "\n",
    "| Function | Description |\n",
    "|--|--|\n",
    "|read_csv| Load delimited data from a file, URL, or file-like object; use comma as default delimiter |\n",
    "|read_table| Load delimited data from a file, URL, or file-like object; use tab ( '\\t' ) as default delimiter |\n",
    "|read_fwf | Read data in fixed-width column format (i.e., no delimiters) |\n",
    "|read_clipboard | Version of read_table that reads data from the clipboard; useful for converting tables from web pages|\n",
    "|read_excel|Read tabular data from an Excel XLS or XLSX file |\n",
    "|read_hdf | Read HDF5 files written by pandas |\n",
    "|read_html | Read all tables found in the given HTML document |\n",
    "|read_json | Read data from a JSON (JavaScript Object Notation) string representation |\n",
    "|read_msgpack | Read pandas data encoded using the MessagePack binary format |\n",
    "|read_pickle | Read an arbitrary object stored in Python pickle format |\n",
    "|read_sas | Read a SAS dataset stored in one of the SAS system’s custom storage formats |\n",
    "|read_sql | Read the results of a SQL query (using SQLAlchemy) as a pandas DataFrame |\n",
    "|read_stata | Read a dataset from Stata file format |\n",
    "|read_father |Read the Feather binary file format |\n",
    "\n",
    "All of these funciton are meant to convert text data into a DataFrame. The optional arguments for these methods can be categorized\n",
    "as:\n",
    "\n",
    "> Indexing : Can treat one or more columns as the returned DataFrame, and whether to get\n",
    "    column names from the file, the user, or not at all.\n",
    "    \n",
    "> Type inference and data conversion : This includes the user-defined value conversions and custom list of missing value\n",
    "    markers    \n",
    "    \n",
    "> Datetime parsing : Includes combining capability, including combining date and time information spread over multiple columns into a single column in the result.\n",
    "\n",
    "> Iterating : Support for iterating over chunks of very large files\n",
    "\n",
    "> Unclead data issues : Skipping rows or a footer, comments, or other minor things like numeric data with thousands separated by commas.\n",
    "\n",
    "---\n",
    "\n",
    "Let's focus on the most basic version of the function.\n",
    "\n",
    "You can read it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('ex1.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default pandas think the first row is the one containing the name\n",
    "of each column. \n",
    "\n",
    "Also, by default pandas uses ',' as the columns separating character\n",
    "\n",
    "We can prevent this behavior in different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(Data('ex1.csv'), header=None) # the first row is considered as part of the data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# this list contains the names of the column\n",
    "# the first row is still considered as part of the data   \n",
    "df2 = pd.read_csv(Data('ex1.csv'), \n",
    "                  names=[\"a1\", \"a2\", \"a3\", \"a4\", \"a5\"])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a csv file with column separated by a tab, such as the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!cat data/ex2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either use ``pd.read_table`` or ``pd.read_csv``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(Data('ex2.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('ex2.csv'), sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible which column will contain the index of the imported dataframe.\n",
    "\n",
    "For instance, if we want the column ``message`` to be the index of the DataFrame,\n",
    "we must issue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "names = ['a', 'b', 'c', 'd', 'message']\n",
    "df = pd.read_csv(Data('ex2.csv'), names=names, sep=\"\\t\", index_col='message')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(hrule(50))\n",
    "# of course we can use\n",
    "print(df.loc['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: There is something wrong with the previous instruction.\n",
    "The first row is wrongly interpreted as part of the data.\n",
    "You can prevent this behavior with the ``skiprows`` argument. \n",
    "\n",
    "``skiprows`` accept a list of integer, each of whom represent \n",
    "the position of a row want to skip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('ex2.csv'), names=names, sep=\"\\t\", index_col='message', skiprows=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas' ``read_csv`` function has almost 50 different arguments. We do not have time for that! \n",
    "\n",
    "The following table summarizes some frequently used options related to both ``read_csv`` and ``read_table``.\n",
    "\n",
    "|Argument | Description|\n",
    "|--|--|\n",
    "| path | String indicating file system location, URL, or file-like object | \n",
    "|sep, delimiter | Character sequence or regular expression to use to split fields in each row |\n",
    "| header | Row number to use as column names; defaults to 0 (first row), but should be None if there is no header row |\n",
    "|index_col | Column numbers or names to use as the row index in the result; can be a single name/number or a list of them for a hierarchical index |\n",
    "| names | List of column names for result, combine with header=None |\n",
    "|skiprows | Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip. |\n",
    "|na_values | Sequence of values to replace with NA. |\n",
    "|comment | Character(s) to split comments off the end of lines. |\n",
    "|parse_dates | Attempt to parse data to datetime ; False by default. If True , will attempt to parse all columns. |\n",
    "|keep_date_col | If joining columns to parse date, keep the joined columns; False by default. |\n",
    "|converters |  Dict containing column number of name mapping to functions (e.g., {'foo': f} would apply the function f to all values in the 'foo' column). |\n",
    "|dayfirst | When parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -> June 7, 2012); False by default. |\n",
    "|date_parser |  Function to use to parse dates. |\n",
    "|nrows | Number of rows to read from beginning of file. |\n",
    "|iterator | Return a TextParser object for reading file piecemeal. |\n",
    "|chunksize | For iteration, size of file chunks. | \n",
    "|skip_footer | Number of lines to ignore at end of file. |\n",
    "|verbose | Print various parser output information, like the number of missing values placed in non-numeric columns.|\n",
    "|encoding | Text encoding for Unicode (e.g., 'utf-8' for UTF-8 encoded text).|\n",
    "|squeeze | If the parsed data only contains one column, return a Series |\n",
    "|thousands | Separator for thousands (e.g., ',' or '.' ). |\n",
    "\n",
    "\n",
    "### Writing Data\n",
    "\n",
    "It is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(Data('ex2.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(Data('ex2_.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the first column! By default pandas saves the index of the dataframe along with the data.\n",
    "\n",
    "You can prevent this behavior as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# index\n",
    "df.to_csv(Data('ex2_.csv'), index=False)\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a different separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(Data('ex2_.csv'), index=False, sep=\";\")\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to skip the column names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(Data('ex2_.csv'), index=False, sep=\";\", header=False)\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON\n",
    "\n",
    "JSON (short for JavaScript Object Notation) has become one of the standard formats\n",
    "for sending data by HTTP request between web browsers and other applications. It is\n",
    "a much more free-form data format than a tabular text form like CSV. Here is an\n",
    "example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "[\n",
    "{\"a\": 1, \"b\": 2, \"c\": 3},\n",
    "{\"a\": 4, \"b\": 5, \"c\": 6},\n",
    "{\"a\": 7, \"b\": 8, \"c\": 9}\n",
    "]\n",
    "\"\"\"\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a JSON object is very close to a built-in python dict.\n",
    "\n",
    "In fact you can easily convert the two objects as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(obj)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert it back as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "asjson = json.dumps(result)\n",
    "#store it into a file\n",
    "with open(Data('ex.json'), 'w') as f:\n",
    "    f.write(asjson)\n",
    "!cat data/ex.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``pandas.read_json`` can automatically convert JSON datasets in specific arrangements into a Series or DataFrame. \n",
    "\n",
    "The default options for pandas.read_json assume that each object in the JSON array\n",
    "is a row in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(Data('ex.json'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to export data from pandas to JSON, one way is to use the ``to_json`` methods on Series and DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if no path is specified the method return \n",
    "# the dataframe as a string\n",
    "record_orientation = df.to_json(orient='records')\n",
    "record_orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index_orientation = df.to_json(orient='index')\n",
    "print(\"record:{}\\nindex:{}\".format(record_orientation, index_orientation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Web APIs\n",
    "\n",
    "Many websites have public APIs providing data feeds via JSON or some other format.\n",
    "There are a number of ways to access these APIs from Python; one easy-to-use\n",
    "method is the ``request`` module.\n",
    "\n",
    "Imagine you have to download a dataset from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_data(url):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return pd.read_csv(StringIO(resp.text))\n",
    "    return None\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
    "df = download_data(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier way to do it is to pass the url directly to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(url).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "A significant amount of your time as a data scientist will be spent \n",
    "on data preparation.\n",
    "\n",
    "The task of preparing the data involves actions like: loading, cleaning, \n",
    "transforming, and so on.\n",
    "\n",
    "Usually, these tasks take almost the 80% of the working hours in a\n",
    "data mining project.\n",
    "\n",
    "### Handling Missing Data\n",
    "\n",
    "One of the goals\n",
    "of pandas is to make working with missing data as painless as possible. \n",
    "\n",
    "In fact, all the summary statistics we computed earlier do not account for missing data.\n",
    "They are automatically ignored by pandas when computing the function.\n",
    "\n",
    "It should be noted that a ``NaN`` has a different meaning if compared to\n",
    "the numpy ``NaN``. \n",
    "\n",
    "In fact, while in numpy it means not-a-number, here in pandas it stands for\n",
    "**Not-Available**.\n",
    "\n",
    "For this reason ``None`` is interpreted as a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy import NaN as than\n",
    "obj = Series([None, 'better', than, 'Kobe'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Select nan values\"))\n",
    "nulls = obj.isnull() # return a mask pointing to every missing value\n",
    "describe(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the built-in *None* is treated as a *None*.\n",
    "\n",
    "#### Filtering Missing Data\n",
    "\n",
    "While filtering missing data we can either use the mask returned by the ``isnull``\n",
    "(there is also a method ``notnull``) or use the ``dropna`` function.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "not_null = obj.dropna() # it returns a copy\n",
    "describe(not_null) # the row at index 2 has been removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:  an equivalent method for removing null values from obj\n",
    "\n",
    "Hint: Boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(\n",
    "    obj[~obj.isnull()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``dropna`` works also with dataframes. The only difference with respect to Series is that you\n",
    "need also to provide the axis along which you want to perform the operation.\n",
    "\n",
    "You can either remove rows or columns (entirely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(np.random.randint(0,50,12).reshape(4, 3), columns=list('abc'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "df = df.astype(np.float32) # convert the dataset to floating point\n",
    "df.values[[0,1,2], [0,2,0]] = than\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default pandas remove an entire record (a row or column) as it finds even just a single missing value.\n",
    "\n",
    "As a consequence this single missing value can cause a massive loss of information. \n",
    "You override this behavior by specifying a different value for the ``how`` argument of dropna (by default it is ``'any'``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with *how='all'* you are telling to pandas that an entry can be regarded as NaN only if all the attributes have a NaN value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than removing entire records we can decide to fill the missing values.\n",
    "\n",
    "The instruction is ``fillna``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    " df.fillna(\"a\").info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a'].fillna(0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "Duplicate rows may be found in a DataFrame for a number of reasons. \n",
    "\n",
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'], 'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "duplicated = df.duplicated() # return a mask \n",
    "duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``drop_duplicates`` methods remove\n",
    "all the duplicates in a DataFrame.\n",
    "``drop_duplicates`` returns a new DataFrame whose value correspond to the \n",
    "value of the original DataFrame where the duplicated Series is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates() # the six entry is gone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:  The membership is evaluated considering the entire row.\n",
    "It means that two entries, with values $\\langle e^i_0, e^i_1 \\dots e^i_n \\rangle$,\n",
    "$\\langle e^j_0, e^j_1 \\dots e^j_n \\rangle$ are considered equal if their values\n",
    "match on every attributes.\n",
    "\n",
    "You can overwrite this behavior by specifying a subset of columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['k1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a duplicated value is found, pandas always keeps the first occurrence of the value.\n",
    "\n",
    "You can override this behavior as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(['k1'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data Using a Function or Mapping\n",
    "Let's assume we want to perform some transformation based on values contained\n",
    "into a data structure.\n",
    "\n",
    "Consider the following hypothetical situation.\n",
    "These are data about several type of meat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'],'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
    "meat_to_animal = {\n",
    "    'bacon': 'pig',\n",
    "    'pulled pork': 'pig',\n",
    "    'pastrami': 'cow',\n",
    "    'corned beef': 'cow',\n",
    "    'honey ham': 'pig',\n",
    "    'nova lox': 'salmon'\n",
    "}\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you need to add a column representing the animal.\n",
    "\n",
    "The ``map`` method comes in handy. \n",
    "It is called on a Series and it accepts a function or dict-like object containing a \n",
    "mapping. \n",
    "However, we have a small problem in that some of the meats are capitalized and\n",
    "others are not.\n",
    "\n",
    "Thus, we need to convert each value to lowercase using the ``str.lower``\n",
    "Series method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data['animal'] = data.food.str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass the dict ``meat_to_animal`` to the map function over the desired column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data['animal'] = data['animal'].map(meat_to_animal)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the ``map`` method can be used for replacing values in a DataFrame there is \n",
    "a more suitable way of doing it, i.e., via the ``replace`` function.\n",
    "\n",
    "You can pass either a pair of scalar values, a pair of array-like values or a dict.\n",
    "\n",
    "The replace function, unless you pass ``inplace=True``, always returns a copy of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obj = Series([1., -999., 2., -999., -1000., 3.])\n",
    "obj_ = obj.replace(-999, NaN)\n",
    "obj_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the replacement is pairwise\n",
    "obj.replace([-999, 2], [-than, -2], inplace=True)\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#the arguments don not need to have the same size\n",
    "obj.replace([-than, -2],-10, inplace=True)\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "                 \n",
    "# you can also pass a dict\n",
    "obj.replace({-10:-99999}, inplace=True)\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = DataFrame(np.random.randint(0,50,12).reshape(4, 3), columns=list('abc'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "df = df.astype(np.float32) # convert the dataset to floating point\n",
    "df.values[[0,1,2], [0,2,0]] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a new column 'd' where\n",
    "there is 'Missing' if \n",
    "the corresponding row contains\n",
    "a Nan in the column 'a', otherwise\n",
    "'d' will contain the value 'not missing'\n",
    "\n",
    "Hint: np.where\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning\n",
    "\n",
    "Continuous data is often subject a process of discretization, i.e., \n",
    "the continuous value is divided into separate bins.\n",
    "\n",
    "Suppose you have data about a group of people in a study, and you want to group\n",
    "them into discrete age buckets.\n",
    "\n",
    "There is the function ``cut``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "cats = pd.cut(ages, bins)\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.column_stack([ages, cats]), columns=['Age', 'Bin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins are represented via a pandas Categorical object. \n",
    "\n",
    "It can be treated like an array of strings.\n",
    "\n",
    "In addition to the name/label associated with each bin, it also stores the interval \n",
    "every bin refers to.\n",
    "\n",
    "Also, every bin is associated with a certain code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"codes:{}\\ncategories:{}\".format(cats.codes, cats.categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``IntervalIndex`` can be seen as an ordered collection of elements, where each element denotes a range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "intIndex = pd.interval_range(start=0, end=5)\n",
    "print(intIndex)\n",
    "intIndex[0] > intIndex[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the number of element inside each bin as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(cats.value_counts())\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``cut`` operation can be customized.\n",
    ">If you specify ``right=False`` the intervals become closed to the left\n",
    "\n",
    ">With ``labels`` you can specify a custom label for each bin\n",
    "\n",
    ">With the argument ``precision`` you can set the number of decimal values to take into account while ``binning\" the Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# here we want 4 bins\n",
    "df['Bin'] = pd.cut(ages,4,right=False, precision=0) \n",
    "df['Label'] = pd.cut(ages,4, labels=['kid', 'teen', 'young', 'old'], right=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing a list of bins you can just specify the number of bins you wish to divide your data.\n",
    "\n",
    "By default ``cut`` splits the data accordingly to an equal-width strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['Bin'] = pd.cut(df['Age'], 5, precision=0)\n",
    "df['Label'] = pd.cut(df['Age'], 5, precision=0)\n",
    "\n",
    "df_ = df.groupby('Bin').count().reset_index().iloc[:, 0:2].rename(columns={'Age': 'Count'})\n",
    "_ = sns.barplot(x='Bin', y='Count', data=df_, label='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if you wish to split the data in quantiles you can use the ``qcut`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].astype(int)\n",
    "df['Bin'] = pd.qcut(df['Age'], 5, precision=0)\n",
    "df['Label'] = pd.qcut(df['Age'], 5, precision=0)\n",
    "\n",
    "df_ = df.groupby('Bin').count().reset_index().iloc[:, 0:2].rename(columns={'Age': 'Count'})\n",
    "_ = sns.barplot(x='Bin', y='Count', data=df_, label='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Permutation and Random Sampling\n",
    "A Permutation is a random reordering.\n",
    "\n",
    "Computing the permutation of a Series of the rows of a DataFrame is fairly simple.\n",
    "\n",
    "There is the function from numpy ``np.random.permutation``.\n",
    "\n",
    "If you call ``np.random.permutation`` with in a integer $N$, numpy will return\n",
    "an array of number in range $[0, N)$ in random order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# it returns 10 number whitin [0,10) in random order\n",
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Describe a way for computing a permutation of a dataframe using \n",
    "the permutation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Extract a random subset from the dataframe of size =$K=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to extract a set of data from the dataframe is via the method ``take``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.random.permutation(df.shape[0])\n",
    "df.take(indexes)#np.random.permutation(indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is even easier with the method ``sample`` of ``DataFrame``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.sample(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default  pandas samples without replacement.  \n",
    "To override this behavior you can use  ``replace=True``\n",
    "to sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample = df.sample(n=3, replace=True)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Indicator/Dummy Variables/One Hot Encoding\n",
    "\n",
    "When you have categorical data, the most common way to represent it is\n",
    "via indicator variables (also known as Dummy Variables)\n",
    "\n",
    "If a column in a dataframe has $k$ distinct values, using dummy variables it \n",
    "will be represented as a $k$-size vector, where each position corresponds \n",
    "to a value. If there is a 1 in the i-th position of this array, it means that\n",
    "the corresponding sample has the i-th categorical value.\n",
    "\n",
    "The ``get_dummies`` function computes the dummy variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_ = pd.get_dummies(df['key'], prefix='key') \n",
    "df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "Merge the one-hot encoded dataframe with the original dataframe.\n",
    "Hint: The two dataframes share the same index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.join(df_).drop('key', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized String Functions in pandas\n",
    "\n",
    "Cleaning up a messy dataset for analysis often requires a lot of string munging and\n",
    "regularization. \n",
    "\n",
    "Also, to make matters worse, column containing strings may have missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',\n",
    "    'Rob': 'rob@gmail.com', 'Wes': np.nan}\n",
    "data = Series(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any string and regular expression methods can be applied (passing a\n",
    "lambda or other function) to each value using the ``map`` function.\n",
    "\n",
    "However, you will struggle with NaN (null) values. \n",
    "\n",
    "To cope with this, Series has array-oriented methods for string operations that skip NA values. \n",
    "\n",
    "These are accessed through Series’s ``str`` attribute. \n",
    "\n",
    "In this way you can directly access the string underlying each element stored in the Series object without considering\n",
    "the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# after accessing the str attribute you can\n",
    "# perform any string-manipulation function (including regex)\n",
    "print(data.str.contains('gmail')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#extract different information from the string\n",
    "#using regular expressions\n",
    "print(data.str.findall('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#check wheter or not a string repsect a certain pattern\n",
    "print(data.str.match('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#create a dataframe. Each column correspond to a matched group\n",
    "describe(data.str.extract('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Create a data frame with two columns containing the username and the domain of the email address, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second = data[:2].str.split('@')\n",
    "print(first, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def extract(series):\n",
    "    first, second = series.str.split('@')\n",
    "    lst = [first, second]\n",
    "    return pd.DataFrame(lst, columns=['username', 'domain'], index=series.index)\n",
    "describe(extract(data[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table with almost every method for handling string objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Image(Img('tab.png'), width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling: Join, Combine, and Reshape\n",
    "### Hierarchical Indexing\n",
    "Hierarchical indexing enables the opportunity of having multiple index levels on a single axis.\n",
    "\n",
    "\n",
    "Let’s start with a simple example; create a Series with a list of lists (or arrays) as the index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.Series(np.random.randn(9), index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'], [1, 2, 3, 1, 3, 1, 2, 2, 3]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a series with a multi index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a hierarchically indexed object, you can select subsets of data via partial indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(data['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fancy index on the first index\n",
    "print(data.loc[['b', 'd']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#inner level selection (likewise a 2-dimensional array)\n",
    "print(data.loc['a':'b', 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexing plays an important role in reshaping data and group-based operations.\n",
    "\n",
    "For example, you can re-arrange the data into\n",
    "a ``DataFrame`` using its ``unstack`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = data.unstack()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first-level index becomes the axis-0 index while the inner-index becomes the axis-1 index (the columns).\n",
    "\n",
    "This inverse operation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(df.stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In a DataFrame, both axis can have a hierarchical index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.arange(12).reshape((4, 3)), \n",
    "                     index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
    "                     columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(frame.loc['a',1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame.loc[('b',1),'Ohio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reordering and Sorting Levels\n",
    "\n",
    "Sometimes, you may need to  rearrange the order of the levels on an axis. \n",
    "\n",
    "Also, you may need to sort the data by a certain criterion.\n",
    "\n",
    "The ``swaplevel`` method takes two level numbers or names\n",
    "and returns a new object where the levels provided as input have been swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# associate a label with each index\n",
    "frame.index.names = ('key1', 'key2')\n",
    "frame.columns.names = ['state', 'color']\n",
    "print(frame)\n",
    "print(Hrule(20, \"swap indexes\"))\n",
    "swapped_frame = frame.swaplevel('key1', 'key2')\n",
    "print(swapped_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When swapping, pandas does not automatically sort the values of the data-frame\n",
    "accordingly to the new configuration the index levels.\n",
    "\n",
    "You need to use the *sort_index*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Sort level 0\"))\n",
    "print(swapped_frame.sort_index(level=0))\n",
    "print(Hrule(20, \"Sort lebel 1\"))\n",
    "print(swapped_frame.sort_index(level=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each level corresponds to a particular index.\n",
    "\n",
    "In this case (after the swapping) ``key2`` corresponds to ``level=0`` while\n",
    "key1 corresponds to ``level=1``.\n",
    "\n",
    "Actually, you do not need to name each index.\n",
    "You can swap them by specifying their level.\n",
    "```\n",
    " df.swaplevel(0,1)\n",
    "```\n",
    "**Exercise**: Make ``swapped_frame`` as the original frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Pandas 2.0.0, many descriptive and summary statistics on DataFrame and Series had a level\n",
    "option in which you can specify the level you want to aggregate by on a particular\n",
    "axis.\n",
    "\n",
    "print(frame.sum(level=1))\n",
    "print(hrule(30))\n",
    "\n",
    "print(frame.mean(level='color', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still emulate this behavior as follows by means of aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(frame.groupby(level=1).sum())\n",
    "print(hrule(30))\n",
    "\n",
    "print(frame.groupby(level='color', axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing with a DataFrame Column \n",
    "Sometimes, you need to use the values contained in one or more column \n",
    "as the index of your dataframe.\n",
    "\n",
    "You need to use the ``set_index`` method. Which asks for the\n",
    "column you want to use as the index (potentially multi-index) of your dataframe.\n",
    "\n",
    "This function returns a copy of the original dataset unless you specify the ``inplace=True``.\n",
    "\n",
    "Here’s an example DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'a': range(7), 'b': range(7, 0, -1),\n",
    "    'c': ['one', 'one', 'one', 'two', 'two',\n",
    "    'two', 'two'],\n",
    "    'd': [0, 1, 2, 0, 1, 2, 3]})\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "frame_ = frame.set_index(['a','b'])\n",
    "print(frame_)\n",
    "print(hrule(20))\n",
    "frame_ = frame.set_index(['c', 'd']).sort_index(level=[0,1], ascending=[True, False])\n",
    "print(frame_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the columns selected as index are removed from the DataFrame\n",
    "However you can keep them specifying:\n",
    "```\n",
    "frame.set_index(['c', 'd'], drop=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and Merging Datasets\n",
    "Data contained in pandas objects can be combined together in a number of ways:\n",
    "\n",
    "1. ``pandas.merge`` connects rows in DataFrames based on one or more keys. This will be familiar to users of SQL or other relational databases, as it implements database join operations.\n",
    "2. ``pandas.concat`` concatenates or “stacks” together objects along an axis.\n",
    "3. ``combine_first`` enables splicing together overlapping data to fill in missing values in one object with values from another.\n",
    "\n",
    "---\n",
    "\n",
    "Merge or join operations combine datasets by linking rows using one or more keys.\n",
    "\n",
    "These operations are central to relational databases (e.g., SQL-based). \n",
    "\n",
    "The merge function in pandas is the main entry point for using these algorithms on your data.\n",
    "Let’s start with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a'],\n",
    "    'data1': range(5)})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a **many-to-one** join;\n",
    "\n",
    "Calling merge with these objects we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2, on='key') # the key column must be present on every data-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It the columns upon which we need to perform a join are named differently in the two datasets,\n",
    "we can specify them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "df1 = df1.rename(columns={'key': 'lkey'})\n",
    "df2 = df2.rename(columns={'key': 'rkey'})\n",
    "\n",
    "pd.merge(df1, df2, left_on='lkey', right_on='rkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that by default pandas performs an inner join, therefore\n",
    "the joint dataframe includes only the rows involved in the join operation.\n",
    "\n",
    "You can override this behavior with the ``how`` argument. \n",
    "\n",
    "It represents the strategy according to which the join is performed.\n",
    "\n",
    "There are 4 possible choice: \n",
    "1. inner \n",
    "2. outer \n",
    "3. left\n",
    "4. right\n",
    "\n",
    "---\n",
    "**Many-to-many** merges have well-defined, though not necessarily intuitive, behavior.\n",
    "Here’s an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],'data2': range(5)})\n",
    "print(df1)\n",
    "print(hrule(30))\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-many joins form the Cartesian product of the rows.\n",
    "\n",
    "Since there were three\n",
    "'b' rows in the left DataFrame and two in the right one, there are six 'b' rows in the\n",
    "result. \n",
    "\n",
    "The join method only affects the distinct key values appearing in the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on='key', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to  perform the merging operation upon the actual keys of the data frame. \n",
    "\n",
    "In such scenario you must pass to the function merge the following arguments: \n",
    "```\n",
    "    pd.merge(..., left_index=True, right_index=True)\n",
    "```\n",
    "By default, i.e., with how='inner', you will obtain the intersection of the two frames.\n",
    "\n",
    "**Example**: compute the union of df1 and df2 wrt to their indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = DataFrame(dict(key=list('abaabc'), value=range(6)))\n",
    "df2 = DataFrame({'key': list('abbcdea'), 'value':range(7)})\n",
    "df1, df2 = df1.set_index('key'), df2.set_index('key')\n",
    "\n",
    "print(df1)\n",
    "print(hrule(30))\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, left_index=True, right_index=True, how = 'outer')# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame has also a convenient ``join`` instance for merging by index. \n",
    "\n",
    "It can also be used\n",
    "to combine together many DataFrame objects having the same or similar indexes **but\n",
    "non-overlapping columns**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "left = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],index=['a', 'c', 'e'],columns=['Ohio', 'Nevada'])\n",
    "right = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])\n",
    "\n",
    "print(pd.merge(left, right, left_index=True, right_index=True, how='outer'))\n",
    "print(Hrule(20, \"Is equivalent to\"))\n",
    "print(left.join(right, how='outer'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table containing all the possible merge arguments\n",
    "\n",
    "|Argument|Description|\n",
    "|--|--|\n",
    "|left|DataFrame to be merged on the left side.|\n",
    "|right|DataFrame to be merged on the right side.|\n",
    "|how|One of 'inner' , 'outer' , 'left' , or 'right' ; defaults to 'inner'|\n",
    "|on|Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys |\n",
    "|left_on | Columns in left DataFrame to use as join keys.|\n",
    "|right_on | Analogous to left_on for left DataFrame.|\n",
    "|left_index | Use row index in left as its join key (or keys, if a MultiIndex).|\n",
    "|right_index | Analogous to left_index. |\n",
    "| sort | Sort merged data lexicographically by join keys; True by default (disable to get better performance in some cases on large datasets)|\n",
    "| suffixes | Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y') (e.g., if 'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result).|\n",
    "| copy | If False , avoid copying data into resulting data structure in some exceptional cases; by default always copies.|\n",
    "| indicator | Adds a special column _merge that indicates the source of each row; values will be 'left_only' , 'right_only' , or 'both' based on the origin of the joined data in each row.|\n",
    "\n",
    "---\n",
    "\n",
    "### Concatenating Along an Axis\n",
    "In the context of pandas objects such as Series and DataFrame, having labeled axes\n",
    "enable you to further generalize array concatenation. In particular, you have a num‐\n",
    "ber of additional things to think about:\n",
    "1. If the objects are indexed differently on the other axes, should we combine the distinct elements in these axes or use only the shared values (the intersection)?\n",
    "2. Do the concatenated chunks of data need to be identifiable in the resulting object?\n",
    "3. Does the “concatenation axis” contain data that needs to be preserved? In many cases, the default integer labels in a DataFrame are best discarded during concatenation\n",
    "\n",
    "The concat function in pandas provides a consistent way to address each of these\n",
    "concerns. \n",
    "\n",
    "Suppose we have\n",
    "three Series with no index overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s1 = Series([0, 1], index=['a', 'b'])\n",
    "s2 = Series([2, 3, 4], index=['c', 'd', 'e'])\n",
    "s3 = Series([5,6], index=['f', 'g'])\n",
    "\n",
    "print(pd.concat([s1, s2, s3])) # by default concat works on axis 0\n",
    "print(hrule(20))\n",
    "print(pd.concat([s1,s2,s3], axis=1, sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Non-overlapping indexes are filled with NaN (it is like an outer join).\n",
    "\n",
    "If you want to retain only the common values you can do it with the *join* argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s4 = Series([0,2,4], index=['a', 'b', 'e'])\n",
    "pd.concat([s1, s4], axis=1,  sort=False, join='inner') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you need to identify the concatenated piece in the result, you can create a hierarchical index as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result = pd.concat([s1, s2, s3], keys=['s1', 's2', 's3'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation on axis=1 produce a DataFrame where each column is named after the element in keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result = pd.concat([s1, s2, s3], keys=['s1', 's2', 's3'], axis=1, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same logic applies on DataFrame as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = DataFrame(np.arange(6).reshape(3,2),index=list('abc'), columns=['one', 'two'])\n",
    "df2 = DataFrame(np.arange(4).reshape(2,2),index=list('ac'), columns=['three', 'four'])\n",
    "print(df1)\n",
    "print(hrule(20))\n",
    "print(df2)\n",
    "print(hrule(20))\n",
    "print(pd.concat([df1,df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'], sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you just need to concatenate multiple dataset without concerning about the the index, because it\n",
    "could provide irrelevant information.\n",
    "\n",
    "In this case you must specify the argument *ignore_index=True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = DataFrame(np.arange(10).reshape(2,5), columns=list('abcde'))\n",
    "df2 = DataFrame(np.arange(10).reshape(2,5), columns=list('abcdf'))\n",
    "\n",
    "pd.concat([df1, df2], axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and Pivoting\n",
    "\n",
    "#### With Hierarchical Indexing\n",
    "There are tow main instructions:\n",
    "1. stack: pivots from the columns to the rows\n",
    "2. unstack : pivots from the rows to the columns (inverse of stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = DataFrame(np.arange(6).reshape(2,3), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number'))\n",
    "print(data)\n",
    "print(Hrule(20, \"Stacking\"))\n",
    "\n",
    "# produce a dataframe with a hierarchical index (state, number)\n",
    "stacked = data.stack()\n",
    "print(stacked)\n",
    "# accessing element\n",
    "print(\"Ohio->one:{}\".format(stacked.loc['Ohio', 'one']))\n",
    "print(Hrule(20, \"Unstacking\"))\n",
    "\n",
    "unstacked = stacked.unstack() # return in the original form\n",
    "print(unstacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the innermost level is stacked (or unstacked). \n",
    "\n",
    "However, you can modify this behavior by sepcifying the level of the index explictly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(stacked.unstack(0)) # the outer index becomes the column index\n",
    "print(Hrule(20, \"See the difference?\"))\n",
    "print(stacked.unstack(1)) # the inner index becomes the column index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting\n",
    "Return reshaped DataFrame organized by given index / column values.\n",
    "\n",
    "Reshape data (produce a “pivot” table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('nba.csv'), usecols=['PLAYER', 'POSITION', 'SALARY_MILLIONS', 'TEAM'], nrows=100)\n",
    "df.drop_duplicates(subset='PLAYER', keep='first', inplace=True)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#pivoting\n",
    "pivoted = df.pivot(index='POSITION',  columns=\"PLAYER\", values=\"SALARY_MILLIONS\")\n",
    "pivoted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melting\n",
    "An inverse operation to pivot for DataFrames is pandas.melt . Rather than transforming one column into many in a new DataFrame, it merges multiple columns into\n",
    "one, producing a DataFrame that is longer than the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "melted = pd.melt(df, ['TEAM'])\n",
    "melted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column becomes a value under the 'variable' column of the melted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"df columns: {} vs melted variable values: {}\".format(df.columns.values,  melted['variable'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of using the melt operation would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# consider only the POSITION and PLAYER columns\n",
    "melt = pd.melt(df, id_vars=['TEAM'], value_vars=['POSITION', 'PLAYER'])\n",
    "print(melt.head())\n",
    "print(melt.variable.unique())\n",
    "\n",
    "#actually we can also omit the id_vars argument\n",
    "print(hrule(50))\n",
    "print(pd.melt(df, value_vars=[\"POSITION\", \"PLAYER\"]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation and Group Operations\n",
    "\n",
    "The entire process of data aggregation can be summerized by the following image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Image(Img('sac.png'), width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image represents a three-stages paradigm called *split-apply-combine*.\n",
    "\n",
    "> The first stage of the process is concerned with the splitting of the data into several groups based on some criteria\n",
    "\n",
    "> The second stage is concerned with the application of a function upon the grouped data\n",
    "\n",
    "> The third and final stage asddress the problem of collecting and presenting the final result, i.e, the result obtained by the application of the function performed in the second stage.\n",
    "\n",
    "The first part of the process is performed by the group_by method.\n",
    "\n",
    "It needs a list of columns names, i.e, the columns upon which the grouping operation has to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grouped_object = df.groupby(by=['key1', 'key2'])\n",
    "grouped_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the grouped object you can apply any function. \n",
    "\n",
    "For instance, if you want to compute the size each group into which the datast has been splitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(grouped_object.size())\n",
    "\n",
    "#of course you can do it in one line of code\n",
    "result = df.groupby('key1').mean(numeric_only=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Let's focus on the dataframe *result*. Which is its index? \n",
    "\n",
    "---\n",
    "\n",
    "If you want to prevent this behavior, you can call the `reset_index` method upon the \n",
    "final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result = result.reset_index()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# or equivalently\n",
    "# result = df.groupby('key1', as_index=False).mean()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Over Groups\n",
    "Sometimes, you may need to iterate over the groups generated by the previous stage.\n",
    "\n",
    "The grouped object provides a way of generating a sequence of 2-tuples containg\n",
    "the group name and the group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for name, group in df.groupby('key1'):\n",
    "    print(name)\n",
    "    print(group, type(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#in case of grouping wrt multiple columns\n",
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print(k1,k2)\n",
    "    print(group, type(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Each group is a simple DataFrame, and of course you can treat it as a regular DataFrame object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Column of Subset of Columns\n",
    "Indexing a GroupBy object created from a DataFrame with a column name or array\n",
    "of column names has the effect of column subsetting for aggregation.\n",
    "\n",
    "Here are several ways for selecting subset of columns with the ``group_by`` operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(df.groupby('key1')['data1'].mean())\n",
    "print(Hrule(20, \"Is equivalent to\"))\n",
    "describe(df['data1'].groupby(df['key1']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "describe(df.groupby('key1')[['data2']].mean())\n",
    "print(Hrule(20, \"Is equivalent to\"))\n",
    "describe(df[['data2']].groupby(df['key1']).mean())\n",
    "print(hrule(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The first two objects are Series while the second two objects are DataFrame.\n",
    "\n",
    "The reason underlying this behavior resides in the notation used for selecting the column.\n",
    "\n",
    "In fact, it should be noted that ``df[['col_name]]`` (double square bracket) returns a DataFrame as opposed\n",
    "to ``df[col_name]`` which return a Series object.\n",
    "\n",
    "### Grouping with Dicts and Series\n",
    "Grouping information may exist in a form other than an array. Let’s consider another\n",
    "example DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "people = pd.DataFrame(np.random.randn(5, 5),\n",
    "    columns=['a', 'b', 'c', 'd', 'e'],\n",
    "    index=['Joe', 'Steve', 'Wes', 'Kendrick', 'Travis'])\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that you have knowledge that columns a-b-c form a group A, and columns d-e form another group B.\n",
    "\n",
    "You want to compute some statistic wrt to groups A,B.\n",
    "\n",
    "One way of doing so is to remap each column in order to embrace the group A and B distinction and then apply the grouping operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mapping = {'a': 'A', 'b':'B', 'c':'A', 'd':'A', 'e':'A'}\n",
    "\n",
    "# axis=1 means that we are grouping the columns\n",
    "people.groupby(mapping, axis=1).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Functions\n",
    "\n",
    "Using Python functions is a more generic way of defining a group mapping compared\n",
    "with a dict or Series. \n",
    "\n",
    "Any function passed as a group key will be called once per index\n",
    "value, with the return values being used as the group names. \n",
    "\n",
    "More concretely, consider the previous example (people dataframe),\n",
    "with names as index values.\n",
    "\n",
    "Suppose you want to group by the length of the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for n,g in people.groupby(len):\n",
    "    print(\"name:{}\\ndata:{}\".format(n,g))\n",
    "    print(hrule(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation\n",
    "\n",
    "Aggregations refer to any data transformation that produces scalar values from\n",
    "arrays\n",
    "\n",
    "Many common aggregations, such as those found in the following table,\n",
    "have optimized implementations. However, you are not limited to only this set of\n",
    "methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image(Img('tabagg.png'), width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own aggregation functions, pass any function that aggregates an array to\n",
    "the aggregate or agg method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()\n",
    "print(df)\n",
    "print(hrule(50))\n",
    "print(df.groupby(['key1', 'key2']).agg(peak_to_peak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Wise and Multiple Function Application\n",
    "\n",
    "Instead of using directly methods like *mean, sum....* we can take advantage of the more general\n",
    "aff function.\n",
    "\n",
    "In fact the ``agg(...)`` function accepts a list of function which are perfomed as a pipeline.\n",
    "\n",
    "Let's consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('tips.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you want to compute the mean and the standard deviation \n",
    "grouped by day and smoker. \n",
    "\n",
    "You can do it in one line of ocde as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "df_['sex'] = df_['sex'].astype('category').cat.codes\n",
    "df_['smoker'] = df_['smoker'].astype('category').cat.codes\n",
    "df_['day'] = df_['day'].astype('category').cat.codes\n",
    "df_['time'] = df_['time'].astype('category').cat.codes\n",
    "\n",
    "df_.groupby(['day', 'smoker']).agg(('mean','std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify different funcion for different columns.\n",
    "\n",
    "You just need to specify a list of 2-tuple as (c,f) where c is the name of a column while\n",
    "f is the a function.\n",
    "\n",
    "> **Note**: c is actually the name of the resulting column not the one in the original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_.groupby(['day', 'smoker']).agg([('mean_column', 'mean'),('std_column', np.std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2\n",
    "## Data Visualization with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# set a style\n",
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-white')\n",
    "['tableau-colorblind10', 'fivethirtyeight', 'fast', 'grayscale', 'dark_background', 'seaborn-dark-palette', 'seaborn-muted', 'seaborn-paper', 'classic', 'seaborn-bright', 'seaborn', 'seaborn-poster', 'seaborn-dark', 'seaborn-darkgrid', 'seaborn-talk', 'seaborn-notebook', 'seaborn-whitegrid', 'seaborn-deep', 'bmh', 'ggplot', 'Solarize_Light2', '_classic_test', 'seaborn-colorblind', 'seaborn-pastel', 'seaborn-ticks', 'seaborn-white']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure and Subplots\n",
    "Plots in matplotlib reside within a ``Figure`` object. \n",
    "\n",
    "You can create a new figure with\n",
    "``plt.figure()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a new figure you can set a number of options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "?plt.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figure is like a canvas, upon which you create a plot.\n",
    "\n",
    "In order to do it you have to call the\n",
    "``add_subplot`` method.\n",
    "\n",
    "With the following cell we create a 4x4 grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?fig.add_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure() # take the reference to the active figure\n",
    "# add_subplot(num_of_rows, num_of_columns, index)    \n",
    "ax1 = fig.add_subplot(2,2, 1)\n",
    "ax2 = fig.add_subplot(2,2, 2)\n",
    "ax3 = fig.add_subplot(2,2, 3)\n",
    "ax4 = fig.add_subplot(2,2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each subplot is indexed with a number, starting from 1.\n",
    "\n",
    "Calling ``plt.plot(something)`` automatically \n",
    "draws the on the last figure and subplot you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure() # take the reference to the active figure\n",
    "# add_subplot(num_of_rows, num_of_columns, index)    \n",
    "ax1 = fig.add_subplot(2,2, 1)\n",
    "ax2 = fig.add_subplot(2,2, 2)\n",
    "ax3 = fig.add_subplot(2,2, 3)\n",
    "ax4 = fig.add_subplot(2,2, 4)\n",
    "_ = plt.plot([1,2,3,4,5], 'k--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each call to ``add_subplot`` returns an object of type ``AxesSubplot``.\n",
    "\n",
    "You can draw directly on that object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure() # take the reference to the active figure\n",
    "# add_subplot(num_of_rows, num_of_columns, index)    \n",
    "ax1 = fig.add_subplot(2,2, 1)\n",
    "ax2 = fig.add_subplot(2,2, 2)\n",
    "ax3 = fig.add_subplot(2,2, 3)\n",
    "ax4 = fig.add_subplot(2,2, 4)\n",
    "plt.plot([1,2,3,4,5], 'k--') # draw on ax4 \n",
    "\n",
    "ax1.hist(np.random.normal(0,100, 1000), bins=20, color='k', alpha=0.3)\n",
    "ax3.hist(np.random.randint(0,100, 1000), bins=20, color='k', alpha=0.3)\n",
    "_ = ax2.scatter(np.arange(30), np.arange(30) + 3*np.random.randint(0,30,30))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Or, if you like, you can use the following more compact way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create 4 subplots on a single row\n",
    "fig, axes = plt.subplots(1,4)\n",
    "fig.set_size_inches(10,2) # set the size of the figure\n",
    "# of type Axes\n",
    "\n",
    "axes[0].hist(np.random.normal(0,100, 1000), bins=20, color='k', alpha=0.3)\n",
    "axes[1].scatter(np.arange(30), np.arange(30) + 3*np.random.randint(0,30,30))\n",
    "axes[2].hist(np.random.randint(0,100, 1000), bins=20, color='k', alpha=0.3)\n",
    "_ = axes[3].plot([1,2,3,4,5], 'k--') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subplots can share their (X,Y) axes.\n",
    "\n",
    "It is useful when you need to compare data having the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create 4 subplots\n",
    "fig, axes = plt.subplots(2,2, sharex=True, sharey=True)\n",
    "axes[0,0].hist(np.random.normal(0,100, 1000), bins=20, color='k', alpha=0.3)\n",
    "axes[0,1].scatter(np.arange(30), np.arange(30) + 3*np.random.randint(0,30,30))\n",
    "axes[1,0].hist(np.random.randint(0,100, 1000), bins=20, color='k', alpha=0.3)\n",
    "_ = axes[1,1].plot([1,2,3,4,5], 'k--') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it looks terrible.\n",
    "\n",
    "In fact, sharing the axis should be done only when the data have the same scale.\n",
    "\n",
    "----\n",
    "\n",
    "## Colors, Marker and Line Styles\n",
    "\n",
    "Matplotlib’s main ``plot`` function accepts arrays of X and Y coordinates and optionally\n",
    "a string abbreviation indicating color and line style.\n",
    "\n",
    "For instance, for a simple line-plot with a green dashed-line \n",
    "\n",
    "```\n",
    "ax.plot(x, y, 'g--')\n",
    "```\n",
    "The third argument is a compact way for writing:\n",
    "\n",
    "```\n",
    "ax.plot(x, y, linestyle='--', color='g')\n",
    "```\n",
    "\n",
    "Matplotlib offers several string abbrevations for setting the style of the plot. \n",
    "\n",
    "In this case we are drawing a dashed line with color grey.\n",
    "\n",
    "Moreover, we can also set set the style related to the marker of the line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, sharey=True, sharex=True)\n",
    "fig.set_size_inches(12,3)\n",
    "axes[0].plot(np.random.random(10).cumsum(), 'ko--')\n",
    "_ = axes[1].plot(np.random.random(10).cumsum(), linestyle='dashed', color='k', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  set the space between the subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, any graph must include labels in order to be interpreted the right way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "fig.set_size_inches(4, 4)\n",
    "ax.plot(np.power(np.random.random(10),2), 'ko--')\n",
    "# setting ticks over the axis\n",
    "xticks = ax.set_xticks([0, 5, 10])\n",
    "yticks = ax.set_yticks([0, 0.5, 1])\n",
    "xlabels = ax.set_xticklabels(['zero', 'five', 'ten'], rotation=30, fontsize=24)\n",
    "ylabels = ax.set_yticklabels(['zero', '.5', '1'], fontsize=24)\n",
    "#setting the title \n",
    "ax.set_title(\"Matplotlib\", fontsize=24)\n",
    "#setting the labels over the axes\n",
    "ax.set_xlabel('x', fontsize=24)\n",
    "_ = ax.set_ylabel(r'$y=x^2$', fontsize=24) # set a laTex string as ylabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, imagine you have to plot\n",
    "different series on the same axes. \n",
    "\n",
    "You will need to add a legend/label to each plot.\n",
    "\n",
    "The easiest way is to use the ``label`` argument of the plot function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y1 = np.power(np.random.random(20),2) \n",
    "y2 = np.power(np.random.random(20), 3)\n",
    "y3 = np.power(np.random.random(20), 4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(y1, 'k', label=r'$x^2$')\n",
    "ax.plot(y2, 'k--', label=r'$x^3$')\n",
    "ax.plot(y3, 'k:' , label=r'$x^4$')\n",
    "\n",
    "_ = ax.legend(loc='best', frameon=True, framealpha=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Annotations and Drawing on a Subplot\n",
    "\n",
    "In addition to the standard plot types, you may wish to draw your own plot annotations,\n",
    "e.g., text, arrows, or other shapes.\n",
    "\n",
    "Annotations and text can be added using the ``text`` , ``arrow`` , and ``annotate`` functions.\n",
    "\n",
    "For instance ``text`` draws a given text at the given coordinates (x, y) on the plot with optional custom styling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y = np.arange(10)\n",
    "annotations = [\n",
    "    (5, 'half'), \n",
    "    (0, 'start')\n",
    "]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(y)\n",
    "\n",
    "for x, label in annotations:\n",
    "  ax.annotate(label,\n",
    "              xy=(x, y[x]),  \n",
    "              xytext=(x,y[x]+2),\n",
    "              arrowprops={\"facecolor\":'black'},\n",
    "              horizontalalignment='left',\n",
    "              verticalalignment='top'\n",
    "             )\n",
    "  # drawind a shape\n",
    "  rect = plt.Rectangle(\n",
    "      (x-0.2, y[x]-0.2),  # bottom left corner coordinates\n",
    "      0.4,  # width\n",
    "      0.4, # height\n",
    "      color='k', \n",
    "      alpha=0.3,\n",
    "      fill=False,\n",
    "      linewidth=2\n",
    "   )\n",
    "  ax.add_patch(rect) # add the shape\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with seaborn\n",
    "\n",
    "Seaborn provides a high-level interface to Matplotlib, a powerful but sometimes it may be awkward. \n",
    "\n",
    "On Seaborn’s official website, they state:\n",
    "\n",
    "\n",
    "> “If Matplotlib “tries to make easy things easy and hard things possible”, seaborn tries to make a well-defined set of hard things easy too” \n",
    "\n",
    "How Seaborn compares with matplotlib:\n",
    "\n",
    "1.Matplotlib can be personalized but it’s difficult to figure out what settings are required to make plots more attractive. On the other hand, Seaborn comes with numerous customized themes and high-level interfaces to solve this issue.\n",
    "\n",
    "2.When working with Pandas, Matplotlib doesn’t serve well when it comes to dealing with DataFrames, while Seaborn functions actually work on DataFrames.\n",
    "3. Matplotlib's API is relatively low level. Doing sophisticated statistical visualization is possible, but often requires a lot of boilerplate code.\n",
    "4. Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas DataFrames. In order to visualize data from a Pandas DataFrame, you must extract each Series and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the DataFrame labels in a plot.\n",
    "\n",
    "\n",
    "## Seaborn Versus Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some random walk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0) # set seed of the random generator\n",
    "x = np.linspace(0,10,500)\n",
    "y = np.cumsum(rng.randn(500,6),0)\n",
    "plt.plot(x, y)\n",
    "plt.legend('ABCDEF', ncol=2, loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Although the result contains all the information we'd like it to convey, it does so in a way that is\n",
    "not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of \n",
    "21st-century data visualization.\n",
    "\n",
    "Now let's take a look at how it works with Seaborn. As we will see, Seaborn has many of its own high-level plotting routines,\n",
    "but it can also overwrite Matplotlib's default parameters and in turn get even simple Matplotlib scripts to produce vastly superior output. We can set the style by calling\n",
    "Saborn's set() method. \n",
    "\n",
    "Re-run the same two lines as before, after calling ``sns.set()``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set() # searborn is on\n",
    "plt.plot(x,y)\n",
    "plt.legend('ABCDEF', ncol=2, loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Seaborn Plots\n",
    "The main purpose of Seaborn is to provide a high-level \n",
    "approach to data visualization.\n",
    "\n",
    "Please note that matplotlib allows you to reach the same results, however\n",
    "seaborn gets it easier\n",
    "\n",
    "### Histograms, KDE, and densities\n",
    "These are the most useful type of plot when you need to visualize disrtibutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)\n",
    "data = pd.DataFrame(data, columns=['x', 'y'])\n",
    "\n",
    "for col in 'xy':\n",
    "    plt.hist(data[col], alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a simple histogram we can plot distribution as a kernel density estimation with\n",
    "``sns.kdeplot``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for col in 'xy':\n",
    "    sns.kdeplot(data[col], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and KDE can be combined together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(data['x'])\n",
    "sns.distplot(data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the joint and the marginal distribution with ``sns.jointplot``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    sns.jointplot(data=data, x='x',y='y', kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other parameters that you can pass.\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    sns.jointplot(x='x',y= 'y', data=data, kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plots\n",
    "When you generalize the notion of joint plots to dataset with larger\n",
    "dimension you get a pair plot, where each pair of values are plotted against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue='species', size=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the best way to view data is via\n",
    "histograms of attributes' subset. \n",
    "\n",
    "Seaborn ``FacetGrid`` makes this extremely simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tips['tips_pct'] = 100*tips['tip']/tips['total_bill']\n",
    "grid = sns.FacetGrid(tips, row='sex', col='time', margin_titles=True)\n",
    "grid.map(plt.hist, 'tips_pct', bins=np.linspace(0, 40, 15));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful visualization tool is the catplot (previously known as factor plot).\n",
    "\n",
    "This allows you to view the distribution of a parameter within bins defined by any other parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style='ticks'):\n",
    "    g = sns.catplot(data=tips, x=\"day\", y='total_bill', hue='sex', kind='box')\n",
    "    g.set_axis_labels(\"Day\", \"Total Bill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful way of plotting any pair of variables is via the ``sns.jointplot``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    sns.jointplot(x='total_bill', y='tip', data=tips, kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jointplot can do a density estimation or even an automatic regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Plots\n",
    "Let's load another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "planets = sns.load_dataset('planets')\n",
    "planets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series can be plotted using ``sns.catplot`` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.catplot(x=\"year\", data=planets, aspect=2,\n",
    "                       kind=\"count\", color='steelblue')\n",
    "    g.set_xticklabels(step=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can introduce a category upon which a split of the data will be performed with the ``hue`` parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    g = sns.catplot(x=\"year\", data=planets, aspect=4.0, kind='count',\n",
    "                       hue='method', order=range(2001, 2015))\n",
    "    g.set_ylabels('Number of Planets Discovered')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A case of study: Marathon Data\n",
    "\n",
    "First download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv | mv marathon-data.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(Data('marathon-data.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values contained are unfortunately stored as object. \n",
    "Not very useful.\n",
    "\n",
    "Let's override this behavior by providing a converter to the ``read_csv`` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def time(s):\n",
    "    h,m,s = map(int, s.split(':'))\n",
    "    return pd.Timedelta(hours=h, minutes=m, seconds=s)\n",
    "\n",
    "data = pd.read_csv(Data('marathon-data.csv'),\n",
    "                   converters={'split':time, 'final':time})\n",
    "print(data.dtypes)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert time deltas in seconds to do a favor at seaborn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data['split_sec'] = data['split'].astype(int)/1E9\n",
    "data['final_sec'] = data['final'].astype(int)/1E9\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "split_min, split_max = data.split_sec.min(), data.split_sec.max()\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.jointplot(x=\"split_sec\", y=\"final_sec\", data=data, kind='hex')\n",
    "    g.ax_joint.plot(np.linspace(split_min, split_max),\n",
    "                    np.linspace(2*split_min, 2*split_max), ':k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The dotted line shows where someone's time would lie if they ran the marathon at a perfectly steady pace. The fact that the distribution lies above this indicates (as you might expect) that most people slow down over the course of the marathon. If you have run competitively, you'll know that those who do the opposite—run faster during the second half of the race—are said to have \"negative-split\" the race.\n",
    "\n",
    "Let's create another column in the data, the split fraction, which measures the degree to which each runner negative-splits or positive-splits the race:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data['split_frac'] = 1 - 2 * data['split_sec'] / data['final_sec']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where this split difference is less than zero, the person negative-split the race by that fraction. Let's do a distribution plot of this split fraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(data['split_frac'], kde=False)\n",
    "plt.axvline(0, color=\"k\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sum(data.split_frac < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Out of nearly 40,000 participants, there were only 250 people who negative-split their marathon.\n",
    "\n",
    "Let's see whether there is any correlation between this split fraction and other variables. We'll do this using a pairgrid, which draws plots of all these correlations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(data, vars=['age', 'split_sec', 'final_sec', 'split_frac'],\n",
    "                 hue='gender', palette='RdBu_r')\n",
    "g.map(plt.scatter, alpha=0.8)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It looks like the split fraction does not correlate particularly with age, but does correlate with the final time: faster runners tend to have closer to even splits on their marathon time. (We see here that Seaborn is no panacea for Matplotlib's ills when it comes to plot styles: in particular, the x-axis labels overlap. Because the output is a simple Matplotlib plot, however, the methods in Customizing Ticks can be used to adjust such things if desired.)\n",
    "\n",
    "The difference between men and women here is interesting. Let's look at the histogram of split fractions for these two groups:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(data.split_frac[data.gender=='M'], label='men', shade=True)\n",
    "sns.kdeplot(data.split_frac[data.gender=='W'], label='women', shade=True)\n",
    "plt.xlabel('split_frac');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The interesting thing here is that there are many more men than women who are running close to an even split! This almost looks like some kind of bimodal distribution among the men and women. Let's see if we can suss-out what's going on by looking at the distributions as a function of age.\n",
    "\n",
    "A nice way to compare distributions is to use a violin plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"gender\", y=\"split_frac\", data=data,\n",
    "               palette=[\"lightblue\", \"lightpink\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is yet another way to compare the distributions between men and women.\n",
    "\n",
    "Let's look a little deeper, and compare these violin plots as a function of age. We'll start by creating a new column in the array that specifies the decade of age that each person is in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data['age_dec'] = data.age.map(lambda age: 10 * (age // 10))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "men = (data.gender == 'M')\n",
    "women = (data.gender == 'W')\n",
    "\n",
    "with sns.axes_style(style=None):\n",
    "    sns.violinplot(x=\"age_dec\", y=\"split_frac\", hue=\"gender\", data=data,\n",
    "                   split=True, inner=\"quartile\",\n",
    "                   palette=[\"lightblue\", \"lightpink\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Looking at this, we can see where the distributions of men and women differ: the split distributions of men in their 20s to 50s show a pronounced over-density toward lower splits when compared to women of the same age (or of any age, for that matter).\n",
    "\n",
    "Also surprisingly, the 80-year-old women seem to outperform everyone in terms of their split time. This is probably due to the fact that we're estimating the distribution from small numbers, as there are only a handful of runners in that range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(data.age > 80).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily. We'll use regplot, which will automatically fit a linear regression to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot(x='final_sec', y='split_frac', col='gender', data=data,\n",
    "               markers=\".\", scatter_kws=dict(color='c'))\n",
    "g.map(plt.axhline, y=0.1, color=\"k\", ls=\":\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the people with fast splits are the elite runners who are finishing within ~15,000 seconds, or about 4 hours. People slower than that are much less likely to have a fast second split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "name": "Lecture2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
