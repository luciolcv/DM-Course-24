{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this notebook you will go through a classification problem, through which you are going to predict a categorical value, i.e., a class.\n",
    "\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT_DIR,'data')\n",
    "IMG_DIR = os.path.join(PROJECT_ROOT_DIR, 'img')\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "MNIST is likely the most famous dataset in ML.\n",
    "\n",
    "It is a collection of 70.000 handwritten digits. \n",
    "\n",
    "Each image is associated with its actual digit.\n",
    "\n",
    "`sklearn` provides some helper function to fetch this dataset.\n",
    "\n",
    "**Warning**: `fetch_mldata()` is deprecated since Scikit-Learn 0.20. You should use `fetch_openml()` instead. However, it returns the unsorted MNIST dataset, whereas `fetch_mldata()` returned the dataset sorted by target (the training set and the test test were sorted separately). In general, this is fine, but if you want to get the exact same results as before, you need to sort the dataset using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sort_by_target(mnist):\n",
    "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
    "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
    "    mnist.data[:60000] = mnist.data.iloc[reorder_train]\n",
    "    mnist.target[:60000] = mnist.target.iloc[reorder_train]\n",
    "    mnist.data[60000:] = mnist.data.iloc[reorder_test + 60000]\n",
    "    mnist.target[60000:] = mnist.target.iloc[reorder_test + 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_mnist():\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    download = False\n",
    "    if not os.listdir(DATA_DIR):\n",
    "        download = True\n",
    "        # empty directory download the file\n",
    "        try:\n",
    "            from sklearn.datasets import fetch_openml\n",
    "            mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "            mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "            sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
    "        except ImportError:\n",
    "            from sklearn.datasets import fetch_mldata\n",
    "            mnist = fetch_mldata('MNIST original')\n",
    "    mode = 'wb' if download else 'rb'\n",
    "    with open(os.path.join(DATA_DIR,'mnist.pickle'), mode) as f:\n",
    "        if download:\n",
    "            pickle.dump(mnist, f)\n",
    "        else:\n",
    "            mnist = pickle.load(f)\n",
    "    return mnist\n",
    "\n",
    "mnist = maybe_download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object you just downloaded has a dict-like structure.\n",
    "\n",
    "The actual data are stored in the ``.data`` field of the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print([k for k in mnist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the shape of your dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"].values, mnist[\"target\"].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70.000 images with 784 features. \n",
    "Each feature represents the density of a pixel, ranging from 0 to 255.\n",
    "\n",
    "Each row is the rollout vector of a 28x28 image.\n",
    "\n",
    "You can plot some data by reshaping the vector into the original matrix and then you\n",
    "can use ``matplotlib.imshow``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "some_digit = X[36000]\n",
    "\n",
    "def draw(sample, save_path = None):\n",
    "    some_digit_image = some_digit.reshape(28, 28)\n",
    "    plt.imshow(some_digit_image, cmap = mpl.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    if save_path is not None:\n",
    "        save_fig(os.path.join(IMG_DIR, save_path))\n",
    "\n",
    "draw(some_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=1/7, random_state=42 )\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier\n",
    "\n",
    "Before diving into the details of the problem,\n",
    "let's focus on slightly simpler sub-problem. \n",
    "\n",
    "You have to train a _binary classifier_.\n",
    "\n",
    "The label _5__ is regarded as the positive class.\n",
    "\n",
    "Let's create the new target vector for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "some_digit = X[36000]\n",
    "draw(some_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick the **Stochastic Gradient Descent** classifier and train it upon the \n",
    "new dataset.\n",
    "\n",
    "The main advantage of this classifier is its ability to handle large datasets in a very efficient way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# train it yourself\n",
    "sgd_clf = SGDClassifier(max_iter=5, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures for Classification Problem.\n",
    "\n",
    "\n",
    "**Question**: Which are the most suitable performance measures for this kind of problems?\n",
    "\n",
    "*Answer*: \n",
    "\n",
    "----\n",
    "\n",
    "## Measuring Accuracy with Cross-Validation\n",
    "Accuracy is defined as the ratio of \n",
    "the number of correct predictions on the total number of predictions. \n",
    "\n",
    "More formally:\n",
    "\n",
    "$$\n",
    "ACCURACY = \\frac{\\#\\: right predictions}{\\#\\: predictions} = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "### Implementing Cross-Validation\n",
    "Occasionally you will need more control over the cross-validation process than what\n",
    "``cross_val_score`` and similar functions provide.\n",
    "\n",
    "In these cases, you can implement\n",
    "cross-validation yourself; it is actually fairly straightforward.\n",
    "\n",
    "The ``StratifiedKFold`` class performs stratified sampling \n",
    "to produce folds that contain a representative ratio of each class. \n",
    "\n",
    "Every iteration of the following code involves the following steps:\n",
    "    1. Create a clone of the classifier\n",
    "    2. Train the classifier against the training folds\n",
    "    3. Compute predicitons over the validation fold \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# stratified splitting of the training samples\n",
    "skfolds = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# your code here\n",
    "# Hint1 : call split and iterate over the 3 folds created by skfold\n",
    "# Hint2 : clone the classifier\n",
    "# Hint3 : call fit and then predict\n",
    "# Hint4 : compute and pring the accuracy of the current classifier\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train_5[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train_5[test_index])\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your are lazy, just go with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_scores = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "cross_val_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 90%! It is great, isn't it?\n",
    "\n",
    "**Question**: Should we be satisfied with this result?\n",
    "\n",
    "Let's implement another classifier.  A dummy classifier that always predict\n",
    "``False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5.sum()/y_train_5.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What's just happened?\n",
    "\n",
    "Accuracy is often a misleading metric,\n",
    "especially when data are skewed.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the confusion matrix.\n",
    "\n",
    "The general idea is to count the number of times instances of class A are\n",
    "classified as class B.\n",
    "\n",
    "The confusion matrix can be computed with function ``confusion_matrix`` of\n",
    "``sklearn.metrics``.\n",
    "\n",
    "First you need to compute some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def display_cofusion_matrix(matrix, classes):\n",
    "    s = \"\\t\"+\"\\t\".join(classes)\n",
    "    for i, row in enumerate(matrix):\n",
    "        s += \"\\n{}\\t{}\".format(classes[i], \"\\t\".join([str(x) for x in row]))\n",
    "    return s\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"Ground Truth:{y_train_5.sum()}, Predicted: {y_train_pred.sum()}\")\n",
    "print(display_cofusion_matrix(confusion_matrix(y_train_5, y_train_pred), ['not-5', '5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the actual class while the column represents the predicted class.\n",
    "\n",
    "Since we are dealing with binary classification, we can devise 4 categories:\n",
    "* True Positive - samples predicted as positive that are actually positive (right classification)\n",
    "* True Negative - samples predicted as negative that are actually negative (right classification)\n",
    "* False Positive - samples predicted as positive that are actually negative (wrong classification)\n",
    "* False Negative - samples predicted as negative that are actually positibe( wrong classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_train_5, y_train_pred)).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect binary classifier would be the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "The confusion matrix is the building block for two other measures:\n",
    "\n",
    "* Precision - it is given by: \n",
    "    $$PRECISION = \\frac{TP}{TP+FP}$$\n",
    "    \n",
    "    It measures how many times\n",
    "    prediction of a positive sample were actually positive.\n",
    "    \n",
    "* Recall - it is given by: $$ RECALL = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "    It measures how many positve samples are captured from the set of all the positive samples.\n",
    "    \n",
    "If we apply these measures over the trained classifier we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? The model is far from being perfect as the >90% precision suggested before.\n",
    "\n",
    "Precision and recall, combined together, define the so called $F_1$ score denoted as:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = 2 \\times \\frac{precision \\times recall}{precision +  recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F_1$ score favors classifiers that have similar precision and recall. \n",
    "\n",
    "**Question**: Are precision and recall equally important?\n",
    "\n",
    "*Answer*....\n",
    "\n",
    "**Question**: Is there any correlation between precision and recall?\n",
    "\n",
    "*Answer*....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Precision/Recall Trade-off\n",
    "\n",
    "``SGDClassifier`` associates to each point a score, \n",
    "based on which it takes its final decision about the class of a sample.\n",
    "\n",
    "In order to take this decision, sgd uses a threshold value. \n",
    "If the score of a sample exceeds this threshold then it is considered\n",
    "a positive sample.\n",
    "\n",
    "\n",
    "**Question** How, and to what degree, does the threshold impact on both precision and recall?\n",
    "\n",
    "*Answer*\n",
    "\n",
    "---\n",
    "Let's see what happen when you increase or decrease the threshold.\n",
    "\n",
    "With  the method ``decision_function`` you can access the score assigned to \n",
    "the sample given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(some_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different values of threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "threshold = -300000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to obtain scores instead of classes you can use set ``method=\"decision_function\"``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")\n",
    "y_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting manually a threshold is not a common approach.\n",
    "\n",
    "A better way is to plot the precision recall curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "print(precisions.shape, recalls.shape, thresholds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.xlim([thresholds.min(), thresholds.max()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this curve you can select the threshold that gives you the best \n",
    "precision/recall tradeoff.\n",
    "\n",
    "Another way to select a good precision/recall tradeoff is to plot\n",
    "precision directly against recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > 70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Take Home Lesson__\n",
    "\n",
    "At this point you should be aware of the fact that accuracy is not the only metrics.\n",
    "\n",
    "Clearly, having 90% precision is not useful if either recall or precision are too low.\n",
    "\n",
    "## ROC curves\n",
    "The receiver operating characteristic curve is another common tool used\n",
    "with binary classifier.\n",
    "\n",
    "The ROC curve shows the true-positive-rate (another name for Recall) against the false-positive-rate.\n",
    "\n",
    "This second quantity, the  FPR, is the ratio of negative samples misclassified as positive.\n",
    "\n",
    "Another quantity is the true-negative-rate, i.e., the ratio of negative samples correctly classified as negative (TNR)\n",
    "\n",
    "The false positive rate can be computed as $FPR = 1-TNR$\n",
    "\n",
    "The ROC curve combines all these information.\n",
    "\n",
    "In short:\n",
    "\n",
    "* True positive rate (recall / sensitivity)\n",
    "    $$TPR = \\frac{TP}{TP+FN} = 1 - FNR$$\n",
    "\n",
    "* False positive rate \n",
    "    $$\n",
    "        FPR = \\frac{FP}{FP+TN} = 1 - TNR\n",
    "    $$\n",
    "* True negative rate (specificity)\n",
    "    $$\n",
    "        TNR = \\frac{TN}{TN+FP} = 1 - FPR\n",
    "    $$\n",
    "\n",
    "To plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the  ``roc_curve`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the ROC curve highlights the negative correlation between precision and recall.\n",
    "\n",
    "In fact, increasing  recall usually leads to decreasing accuracy.\n",
    "\n",
    "The dotted line represents the ROC curve associated to a purely random classifier. \n",
    "\n",
    "The performance of a classifier are proportional to the distance between its \n",
    "ROC curve and the one related to the random classifiers.\n",
    "\n",
    "\n",
    "One can directly measure the area under the ROC curve, that is called AUC.\n",
    "\n",
    "It must be as closer as possible to one.\n",
    "\n",
    "sklearn provides a function for computing this area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "You have seen a number of different ways for measuring the performance of a classifier. \n",
    "\n",
    "> How to choose the correct one?\n",
    "\n",
    "There are general rule of thumbs.\n",
    "\n",
    "Precision Recall comes in handy when the dataset is unbalanced, with few positive samples\n",
    "as opposed to negative ones. Also, if you are more interested in keeping false positive low\n",
    "rather than false negative.\n",
    "\n",
    "In other situations, ROC/AUC is the way to go.\n",
    "\n",
    "This result suggests that your model works pretty good. \n",
    "\n",
    "However, there is still room for improvement. \n",
    "In fact, performance are good because the dataset is strongly unbalanced towards the negative class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Train a ``RandomForestClassifier`` (Hint: pass ``predict_proba`` as the ``method`` argument).\n",
    "\n",
    "Display the ROC curve, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
    "precision_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "From this point on, we will address the original multiclass classification problem.\n",
    "\n",
    "\n",
    "Instead of having to choose between only a pair of labels, you need to assign a label amongst a number\n",
    "labels.\n",
    "\n",
    "Many algorithm like the Random Forest Classifier\n",
    "or the Naive Bayes Classifier are able to genuinely handle multiclass\n",
    "classification problems.\n",
    "\n",
    "However, other algorithm needs to be properly extends in order to \n",
    "embrace a multiclass classification problem.\n",
    "\n",
    "The main strategy is to adapt their binary version.\n",
    "\n",
    "Two approaches are:\n",
    "\n",
    "    1. One vs All (OVA) - if you are asked to predict a class amongst N different ones, you can train N different binary classifiers. Each one of them is trained in order to detect a single class against all the others.\n",
    "    \n",
    "    2. One vs One (OVO) - given N different class, you can  N x (N-1)/2 different binary classifier. You will need to train a classifier for every possible pair of classes.\n",
    "\n",
    "For most binary classification algorithms the OVA approach is the one adopted by default.\n",
    "\n",
    "sklearn automatically detects when you need to solve a multiclass classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, under the hood,\n",
    "scikit-Learn actually trained 10 binary classifiers, got their decision scores for the\n",
    "image, and selected the class with the highest score.\n",
    "\n",
    "To convince yourself about the actual strategy adopted by sklearn you can \n",
    "compute the scores associated with the ``some_digit``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each score  **reflects** a 'score', i.e., the likelihood the a certain sample belongs to \n",
    "the corresponding class.\n",
    "\n",
    "The class is assigned computing the argmax on this vector of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Be careful, this 5 actually is the index of the maximum element, not the class!\n",
    "\n",
    "The classes upon which the classifier is trained can be accessed via the ``classes_`` field of the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.classes_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can force sklearn to adopt the OVO by wrapping a classifier with the\n",
    "``OneVsOneClassifier`` .\n",
    "You need to pass a regular binary classifier and then call the usual fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(max_iter=5, tol=1e-3, random_state=42))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try with a **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets over 84% on all test folds. \n",
    "\n",
    "A random classifier would get 10% accuracy, not a bad score.\n",
    "But you can improve.\n",
    "\n",
    "We did not use any preprocessing techniques yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "p = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', sgd_clf)\n",
    "])\n",
    "\n",
    "cross_val_score(p, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "Let assume we found a promising model.\n",
    "\n",
    "One thing to do is to evaluate its errors.\n",
    "\n",
    "You need to make predictions using the ``cross_val_predict`` fnction and then\n",
    "call the ``confusion_matrix()`` function, just like\n",
    "you did earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "y_train_pred =  cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_confusion_matrix(matrix):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.heatmap(matrix, \n",
    "                annot=True,  \n",
    "                cmap=\"YlGnBu\")\n",
    "            \n",
    "plot_confusion_matrix(conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s focus the plot on the errors. \n",
    "\n",
    "First, you need to divide each value in the confusion\n",
    "matrix by the number of images in the corresponding class, so you can compare error\n",
    "rates instead of absolute number of errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True) # num of samples for each class\n",
    "col_sum = conf_mx.sum(axis=0, keepdims=True)  # num of predictions for each class\n",
    "norm_conf_mx = conf_mx / row_sums\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s fill the diagonal with zeros to keep only the errors, and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns corresponding to 8 and 9 are quite dark,\n",
    "which tells you that many images get misclassified as 8s or 9s. \n",
    "\n",
    "Similarly, the rows for classes 8 and 9 are also quite dark, telling you that 8s\n",
    "and 9s are often confused with other digits. \n",
    "\n",
    "Conversely, some rows are pretty bright,\n",
    "such as ow 1: this means that most 1s are classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel classification\n",
    "\n",
    "Until now each instance has always been assigned to just one class. In some cases you\n",
    "may want your classifier to output multiple classes for each instance. For example,\n",
    "consider a face-recognition classifier: what should it do if it recognizes several people\n",
    "on the same picture?\n",
    "\n",
    "Such a classification system that outputs multiple\n",
    "binary labels is called a multilabel classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a y_multilabel array containing two target labels for each digit\n",
    "image: the first indicates whether or not\n",
    "the digit is large (7, 8, or 9) and the second\n",
    "indicates whether or not it is odd. The next lines create a KNeighborsClassifier,\n",
    "Now you can make a prediction, and notice\n",
    "that it outputs two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "knn_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\n",
    "\n",
    "There are many ways to evaluate a multilabel classifier, and selecting the right metric\n",
    "really depends on your project. For example, one approach is to measure the F 1 score\n",
    "for each individual label (or any other binary classifier metric discussed earlier), then\n",
    "simply compute the average score. This code computes the average F 1 score across all\n",
    "labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3, n_jobs=-1)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumes that all labels are equally important, which may not be the case.\n",
    "\n",
    "One simple option is to give each label a weight equal to its support (i.e., the number of instances with that\n",
    "target label). To do this, simply set average=\"weighted\" in the preceding code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Train different classifiers and test their performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An MNIST Classifier With Over 97% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors seems to perform fine. Try to push its performance with \n",
    "automated parameter tuning. \n",
    "\n",
    "The algorithm has just one parameter: k, which represents the n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train, y_train)\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Selecting the best Model\n",
    "Now, you are required to build an entire pipeline. \n",
    "You need to select a number of models, tuning upon each of them and then you need to report in a table the following\n",
    "information:\n",
    "\n",
    "1. Name of The estimator\n",
    "2. Best Configuration\n",
    "3. accuracy\n",
    "3. precision\n",
    "4. recall\n",
    "5. AUC score\n",
    "\n",
    "Of course, these results must be obtained wrt the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "name": "lecture6_solutions.ipynb",
  "nav_menu": null,
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
